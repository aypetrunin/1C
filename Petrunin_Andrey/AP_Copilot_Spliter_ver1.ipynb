{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aypetrunin/1C/blob/main/Petrunin_Andrey/AP_Copilot_Spliter_ver1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vtPWszprB39"
      },
      "source": [
        "## Описание"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-iqwt4z2Bpr"
      },
      "source": [
        "В блокноте реализуются две задачи:\n",
        "1. Предварительная обработка текстовой базы знаний.\n",
        "2. Разбивка текстовой базы знаний на чанки.\n",
        "\n",
        "Во время предварительной обработки текстовой базы знаний решаются следующие задачи:\n",
        "1. Проверка и исправление иерархии заголовков.\n",
        "2. Удаление пустых строк.\n",
        "3. Иерархическое расширение заголовков.\n",
        "4. Разбитие длинных разделов на части.\n",
        "5. Перенос одиноких дочерних разделов в родительский с корректировкой расширенного родительского заголовка.\n",
        "6. Перенос коротких родительских разделов в дочерние разделы.\n",
        "7. Приведение всех нумерованных и ненумерованных списков к единому виду нумерованного списка.\n",
        "8. Иерархическая нумерация заголовков.\n",
        "9. Добавление к заголовку информацию о количестве токенов в разделе.\n",
        "10. Копирование заголовка в текст раздела.\n",
        "11. Создание отдельного файла содержания базы знаний.\n",
        "\n",
        "Вся обработка затачивалась под разбивку на первой стадии методом RecursiveCharacterTextSplitter.\n",
        "\n",
        "Во второй задаче проходило исследование различных видов сплиттеров с возможностью деления в два этапа.\n",
        "Контроль равномерности деления проходить через построение диаграммы."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0YgZsxYrH0o"
      },
      "source": [
        "## Подготовка блокнота"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Установка пакетов.\n",
        "\n",
        "!pip install tiktoken\n",
        "!pip install langchain\n",
        "!pip install langchain_openai\n",
        "!pip install langchain_experimental\n",
        "!pip install semantic-text-splitter==0.6.3\n",
        "!pip install sentence_transformers\n",
        "!pip install unstructured\n",
        "\n",
        "from IPython import display\n",
        "# Очистить экран.\n",
        "display.clear_output()"
      ],
      "metadata": {
        "id": "AL5l6u_2LxO1",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iO9CMx_FvOOq",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Импорт библиотек.\n",
        "\n",
        "from langchain.docstore.document import Document\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from semantic_text_splitter import CharacterTextSplitter\n",
        "from semantic_text_splitter import HuggingFaceTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import re\n",
        "import getpass\n",
        "import tiktoken\n",
        "import zipfile\n",
        "import timeit\n",
        "import time\n",
        "from collections import Counter\n",
        "from textwrap import fill\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from typing import List\n",
        "\n",
        "# import gspread                  # Импортируем API для работы с Google таблицами\n",
        "# from google.colab import auth   # Импортируем модуль для аутентификации\n",
        "# from google.auth import default # Импортируем модуль для работы с учетными данными"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9jZlKHIWRsk",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Ввод ключа к API OpenAI.\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6rxZZyP8WZA",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Вспомогательные функции\n",
        "\n",
        "def load_file_of_url(url: str)-> str:\n",
        "    url_google = re.search('docs.google.com', url)\n",
        "    if url_google:\n",
        "        match_ = re.search('/document/d/([a-zA-Z0-9-_]+)', url)\n",
        "        if match_ is None:\n",
        "            raise ValueError('Invalid Google Docs URL')\n",
        "        doc_id = match_.group(1)\n",
        "        response = requests.get(f'https://docs.google.com/document/d/{doc_id}/export?format=txt')\n",
        "    else:\n",
        "        response = requests.get(url) # Получение документа по url.\n",
        "    response.raise_for_status()\n",
        "    return response.text\n",
        "\n",
        "\n",
        "def write_to_file(filename, data):\n",
        "    try:\n",
        "        # Открываем файл в режиме записи\n",
        "        with open(filename, 'w') as file:\n",
        "            # Записываем данные в файл\n",
        "            file.write(data)\n",
        "        # print(f\"Данные успешно записаны в файл '{filename}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при записи данных в файл '{filename}': {e}\")\n",
        "\n",
        "\n",
        "def save_to_file(text, filename):\n",
        "    with open(filename, 'w', encoding='utf-8') as file:\n",
        "        file.write(text)\n",
        "    print(f\"Исправленный текст сохранен в файл {filename}\\n\")\n",
        "\n",
        "\n",
        "def remove_special_characters(input_string):\n",
        "    special_characters = ['#### ','### ','## ','# ', '.', ':', '\\ufeff' ]\n",
        "    for char in special_characters:\n",
        "        input_string = input_string.replace(char, '')\n",
        "    return input_string\n",
        "\n",
        "\n",
        "def extract_structure_with_numbers(filename):\n",
        "\n",
        "    structure = []  # Список для хранения структуры документа с порядковыми номерами\n",
        "    chapter_counter = 0\n",
        "    paragraph_counter = 0\n",
        "    section_counter = 0\n",
        "    point_counter = 0\n",
        "\n",
        "    chapter_title = \"\"\n",
        "    paragraph_title = \"\"\n",
        "    section_title = \"\"\n",
        "    point_title = \"\"\n",
        "\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            if line.startswith('# ') or line.startswith('\\ufeff# '):\n",
        "                # Увеличиваем счетчик глав\n",
        "                chapter_counter += 1\n",
        "                # Сброс счетчиков для параграфов, разделов и пунктов\n",
        "                paragraph_counter = 0\n",
        "                section_counter = 0\n",
        "                point_counter = 0\n",
        "                chapter_title = remove_special_characters(line).strip()\n",
        "                chapter_item = {\"topic\": f\"{chapter_title}. \", \"number\": f\"{chapter_counter}\"}\n",
        "                structure.append(chapter_item)\n",
        "            elif line.startswith('## '):\n",
        "                # Увеличиваем счетчик параграфов\n",
        "                paragraph_counter += 1\n",
        "                # Сброс счетчика для разделов и пунктов\n",
        "                section_counter = 0\n",
        "                point_counter = 0\n",
        "                paragraph_title = remove_special_characters(line).strip()\n",
        "                paragraph_item = {\"topic\": f\"{chapter_title}. {paragraph_title}. \", \"number\": f\"{chapter_counter}.{paragraph_counter}\"}\n",
        "                structure.append(paragraph_item)\n",
        "            elif line.startswith('### '):\n",
        "                section_counter += 1  # Увеличиваем счетчик разделов\n",
        "                point_counter = 0  # Сброс счетчика для пунктов\n",
        "                section_title = remove_special_characters(line).strip()\n",
        "                section_item = {\"topic\": f\"{chapter_title}. {paragraph_title}. {section_title}. \", \"number\": f\"{chapter_counter}.{paragraph_counter}.{section_counter}\"}\n",
        "                structure.append(section_item)\n",
        "            elif line.startswith('#### '):\n",
        "                point_counter += 1  # Увеличиваем счетчик пунктов\n",
        "                point_title = remove_special_characters(line).strip()\n",
        "                point_item = {\"topic\": f\"{chapter_title}. {paragraph_title}. {section_title}. {point_title}. \", \"number\": f\"{chapter_counter}.{paragraph_counter}.{section_counter}.{point_counter}\"}\n",
        "                structure.append(point_item)\n",
        "    return structure\n",
        "\n",
        "\n",
        "def plot_bar_chart(data, labels=None, title=None):\n",
        "    # Создаем объект рисунка и оси\n",
        "    fig, ax = plt.subplots()\n",
        "    # Строим столбчатую диаграмму\n",
        "    ax.bar(range(len(data)), data)\n",
        "    # Добавляем метки на оси x\n",
        "    if labels:\n",
        "        ax.set_xticks(range(len(data)))\n",
        "        ax.set_xticklabels(labels)\n",
        "    # Добавляем заголовок\n",
        "    if title:\n",
        "        ax.set_title(title)\n",
        "    # Отображаем диаграмму\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Функции предобработки текста\n",
        "\n",
        "# Подсчет количества токенов.\n",
        "def count_tokens(text):\n",
        "    encoding = tiktoken.get_encoding('cl100k_base')\n",
        "    tokens = encoding.encode(text)\n",
        "    return len(tokens)\n",
        "\n",
        "\n",
        "# Расширяет заголовки в иерархическом тексте в формате Markdown,\n",
        "# добавляя к каждому заголовку путь от корневого заголовка до текущего.\n",
        "def extend_chapter_titles(markdown_text):\n",
        "    \"\"\"\n",
        "    Расширяет заголовки в иерархическом тексте в формате Markdown, добавляя к каждому заголовку путь от корневого заголовка до текущего.\n",
        "\n",
        "    Аргументы:\n",
        "    markdown_text (str): Иерархический текст в формате Markdown.\n",
        "\n",
        "    Возвращает:\n",
        "    str: Текст с расширенными заголовками.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"***** Расширение заголовков *****\\n\")\n",
        "\n",
        "    # Регулярное выражение для поиска заголовков уровня от 1 до 6 в Markdown\n",
        "    heading_pattern = re.compile(r'^(#{1,6})\\s(.+)', re.MULTILINE)\n",
        "\n",
        "    # Список для хранения текущей иерархии заголовков\n",
        "    current_hierarchy = []\n",
        "\n",
        "    def replace_heading(match):\n",
        "        \"\"\"\n",
        "        Заменяет заголовок, найденный регулярным выражением, расширенной версией заголовка, включающей иерархию.\n",
        "\n",
        "        Аргументы:\n",
        "        match (re.Match): Объект, представляющий найденный заголовок.\n",
        "\n",
        "        Возвращает:\n",
        "        str: Расширенный заголовок.\n",
        "        \"\"\"\n",
        "        nonlocal current_hierarchy\n",
        "\n",
        "        # Определение уровня заголовка по количеству символов '#'\n",
        "        level = len(match.group(1))\n",
        "\n",
        "        # Текст заголовка, очищенный от пробельных символов\n",
        "        title = match.group(2).strip()\n",
        "\n",
        "        # Пропускаем заголовки с пустыми названиями (на всякий случай)\n",
        "        if not title:\n",
        "            return \"\"\n",
        "\n",
        "        # Если текущая иерархия меньше уровня заголовка, добавляем заголовок в иерархию\n",
        "        if len(current_hierarchy) < level:\n",
        "            current_hierarchy.append(title)\n",
        "        else:\n",
        "            # Иначе обрезаем иерархию до текущего уровня и заменяем последний элемент на текущий заголовок\n",
        "            current_hierarchy = current_hierarchy[:level]\n",
        "            current_hierarchy[-1] = title\n",
        "\n",
        "        # Создаем расширенный заголовок, объединяя все заголовки в иерархии через '>'\n",
        "        extended_title = ' > '.join(current_hierarchy)\n",
        "\n",
        "        # Возвращаем строку с уровнем заголовка и расширенным заголовком\n",
        "        return f\"{match.group(1)} {extended_title}\"\n",
        "\n",
        "    # Применяем функцию replace_heading ко всем заголовкам в тексте\n",
        "    extended_text = heading_pattern.sub(replace_heading, markdown_text)\n",
        "\n",
        "    # Возвращаем текст с расширенными заголовками\n",
        "    return extended_text\n",
        "\n",
        "\n",
        "    heading_pattern = re.compile(r'^(#{1,6})\\s(.+)', re.MULTILINE)\n",
        "    current_hierarchy = []\n",
        "\n",
        "    def replace_heading(match):\n",
        "        nonlocal current_hierarchy\n",
        "        level = len(match.group(1))\n",
        "        title = match.group(2).strip()\n",
        "\n",
        "        if not title:  # Пропускаем заголовки с пустыми названиями\n",
        "            return \"\"\n",
        "\n",
        "        if len(current_hierarchy) < level:\n",
        "            current_hierarchy.append(title)\n",
        "        else:\n",
        "            current_hierarchy = current_hierarchy[:level]\n",
        "            current_hierarchy[-1] = title\n",
        "\n",
        "        extended_title = ' > '.join(current_hierarchy)\n",
        "        return f\"{match.group(1)} {extended_title}\"\n",
        "\n",
        "    extended_text = heading_pattern.sub(replace_heading, markdown_text)\n",
        "    return extended_text\n",
        "\n",
        "\n",
        "# Разбивает текст на части, каждая из которых содержит не более max_tokens токенов.\n",
        "# Предпочтительно делит текст по указанным разделителям.\n",
        "def split_text_by_delimiters(text, max_tokens, delimiters):\n",
        "    \"\"\"\n",
        "    Разбивает текст на части, каждая из которых содержит не более max_tokens токенов.\n",
        "    Предпочтительно делит текст по указанным разделителям.\n",
        "\n",
        "    Аргументы:\n",
        "    text (str): Текст для разбиения.\n",
        "    max_tokens (int): Максимальное количество токенов в каждой части.\n",
        "    delimiters (list): Список строк-разделителей, по которым предпочтительно делить текст.\n",
        "\n",
        "    Возвращает:\n",
        "    list: Список частей текста, каждая из которых содержит не более max_tokens токенов.\n",
        "    \"\"\"\n",
        "\n",
        "    # Получаем кодировщик для текстов\n",
        "    encoding = tiktoken.get_encoding('cl100k_base')\n",
        "\n",
        "    # Кодируем текст в токены\n",
        "    tokens = encoding.encode(text)\n",
        "\n",
        "    # Если текст уже меньше или равен максимальному количеству токенов, возвращаем его целиком\n",
        "    if len(tokens) <= max_tokens:\n",
        "        return [text]\n",
        "\n",
        "    # Пытаемся разбить текст по каждому из указанных разделителей\n",
        "    for delimiter in delimiters:\n",
        "        # Разбиваем текст по текущему разделителю\n",
        "        parts = text.split(delimiter)\n",
        "\n",
        "        # Кодируем каждую часть, добавляя разделитель обратно (кроме последней части)\n",
        "        tokenized_parts = [encoding.encode(part + delimiter) for part in parts[:-1]] + [encoding.encode(parts[-1])]\n",
        "\n",
        "        # Проверяем, укладываются ли все части в ограничение по количеству токенов\n",
        "        if all(len(part) <= max_tokens for part in tokenized_parts):\n",
        "            break\n",
        "    else:\n",
        "        # Если ни один из разделителей не подходит, разбиваем текст на части фиксированного размера\n",
        "        part_size = max_tokens\n",
        "        parts = [tokens[i:i + part_size] for i in range(0, len(tokens), part_size)]\n",
        "        return [encoding.decode(part) for part in parts]\n",
        "\n",
        "    # Инициализируем переменные для сборки финальных частей текста\n",
        "    final_parts = []\n",
        "    current_part = \"\"\n",
        "    current_tokens = 0\n",
        "\n",
        "    # Собираем части текста, следя за ограничением на количество токенов\n",
        "    for part in parts:\n",
        "        # Количество токенов в текущей части с добавленным разделителем\n",
        "        part_tokens = len(encoding.encode(part + delimiter))\n",
        "\n",
        "        # Если текущая часть укладывается в ограничение, добавляем её к текущей части\n",
        "        if current_tokens + part_tokens <= max_tokens:\n",
        "            current_part += part + delimiter\n",
        "            current_tokens += part_tokens\n",
        "        else:\n",
        "            # Иначе добавляем текущую часть в финальные части и начинаем новую часть\n",
        "            final_parts.append(current_part)\n",
        "            current_part = part + delimiter\n",
        "            current_tokens = part_tokens\n",
        "\n",
        "    # Добавляем последнюю часть, если она не пустая\n",
        "    if current_part:\n",
        "        final_parts.append(current_part)\n",
        "\n",
        "    # Возвращаем список частей текста\n",
        "    return final_parts\n",
        "\n",
        "\n",
        "# Разделяет длинные главы в markdown тексте на части, если они превышают максимальное количество токенов.\n",
        "# Каждая часть получает новый заголовок, указывающий, что это часть длинной главы.\n",
        "def split_and_extend_long_chapters(markdown_text, max_tokens=1024):\n",
        "    \"\"\"\n",
        "    Разделяет длинные главы в markdown тексте на части, если они превышают максимальное количество токенов.\n",
        "    Каждая часть получает новый заголовок, указывающий, что это часть длинной главы.\n",
        "\n",
        "    Аргументы:\n",
        "    markdown_text (str): Текст в формате markdown для обработки.\n",
        "    max_tokens (int): Максимальное количество токенов в каждой части главы.\n",
        "\n",
        "    Возвращает:\n",
        "    str: Обработанный markdown текст с разделенными главами.\n",
        "    \"\"\"\n",
        "    print(\"***** Разделение разделов на части *****\\n\")\n",
        "\n",
        "    # Регулярное выражение для поиска заголовков в markdown тексте\n",
        "    heading_pattern = re.compile(r'^(#{1,6})\\s(.+)', re.MULTILINE)\n",
        "\n",
        "    # Разделение текста на главы по заголовкам\n",
        "    chapters = heading_pattern.split(markdown_text)[1:]\n",
        "\n",
        "    # Инициализация списка для хранения обработанных частей текста\n",
        "    result = []\n",
        "\n",
        "    # Список разделителей для предпочтительного деления текста\n",
        "    delimiters = [\"\\n\\n\", \"\\n\"]\n",
        "\n",
        "    # Проход по каждой главе\n",
        "    for i in range(0, len(chapters), 3):\n",
        "        heading_level = chapters[i]  # Уровень заголовка (например, \"##\")\n",
        "        heading_title = chapters[i + 1].strip()  # Заголовок главы\n",
        "        chapter_content = chapters[i + 2]  # Содержимое главы\n",
        "        heading_level_title = f\"{heading_level} {heading_title}\"\n",
        "\n",
        "        # Подсчет количества токенов в содержимом главы\n",
        "        tokens_count = count_tokens(chapter_content)\n",
        "\n",
        "        if tokens_count > max_tokens:\n",
        "            print(f\"{heading_level_title} - {tokens_count} токенов\")\n",
        "            # Если количество токенов превышает максимальное, делим содержимое главы на части\n",
        "            parts = split_text_by_delimiters(chapter_content, max_tokens, delimiters)\n",
        "\n",
        "            # Создаем заголовки для каждой части и добавляем их в результат\n",
        "            for idx, part in enumerate(parts):\n",
        "                count_token_part = count_tokens(part)\n",
        "                part_title = f\"{heading_title} - part {idx + 1}\"\n",
        "                result.append(f\"{heading_level} {part_title}\\n{part}\\n\")\n",
        "                print(f\"     {heading_level} {part_title} - {count_tokens(part)} токенов\")\n",
        "        else:\n",
        "            count_token_chapter = count_tokens(chapter_content)\n",
        "            # Если количество токенов не превышает максимальное, добавляем главу целиком\n",
        "            result.append(f\"{heading_level} {heading_title}\\n{chapter_content}\\n\")\n",
        "\n",
        "    # Возвращаем объединенный текст из всех обработанных частей\n",
        "    return ''.join(result)\n",
        "\n",
        "\n",
        "# Объединяет короткие главы в markdown тексте с соседними, если они содержат меньше минимального количества токенов.\n",
        "def merge_short_chapters(markdown_text, min_tokens=256, max_tokens=1024):\n",
        "    \"\"\"\n",
        "    Объединяет короткие главы в markdown тексте с соседними, если они содержат меньше минимального количества токенов.\n",
        "\n",
        "    Аргументы:\n",
        "    markdown_text (str): Текст в формате markdown для обработки.\n",
        "    min_tokens (int): Минимальное количество токенов для главы. Главы с меньшим количеством токенов будут объединены с соседними.\n",
        "\n",
        "    Возвращает:\n",
        "    str: Обработанный markdown текст с объединенными короткими главами.\n",
        "    \"\"\"\n",
        "\n",
        "    # Регулярное выражение для поиска заголовков в markdown тексте\n",
        "    heading_pattern = re.compile(r'^(#{1,6})\\s(.+)', re.MULTILINE)\n",
        "\n",
        "    # Разделение текста на главы по заголовкам\n",
        "    chapters = heading_pattern.split(markdown_text)[1:]\n",
        "\n",
        "    # Инициализация списка для хранения обработанных частей текста\n",
        "    result = []\n",
        "\n",
        "    # Инициализация кодировщика для подсчета токенов\n",
        "    encoding = tiktoken.get_encoding('cl100k_base')\n",
        "\n",
        "    # Проход по каждой главе\n",
        "    for i in range(0, len(chapters), 3):\n",
        "        heading_level = chapters[i]  # Уровень заголовка (например, \"##\")\n",
        "        heading_title = chapters[i + 1].strip()  # Заголовок главы\n",
        "        chapter_content = chapters[i + 2].strip()  # Содержимое главы\n",
        "\n",
        "        # Подсчет количества токенов в содержимом главы\n",
        "        tokens_count = count_tokens(chapter_content)\n",
        "\n",
        "        if tokens_count < min_tokens:\n",
        "            # Если количество токенов меньше минимального, пытаемся объединить с соседними главами\n",
        "            if i + 3 < len(chapters) and len(chapters[i + 3]) == len(heading_level):\n",
        "                # Если следующая глава на том же уровне, объединяем с ней\n",
        "                next_heading_title = chapters[i + 4].strip()\n",
        "                next_chapter_content = chapters[i + 5].strip()\n",
        "                heading_title += f\" + {next_heading_title}\"\n",
        "                chapter_content += f\"\\n{next_chapter_content}\"\n",
        "                chapters[i + 2] = chapter_content\n",
        "                chapters[i + 4] = ''\n",
        "                chapters[i + 5] = ''\n",
        "            elif i - 3 >= 0 and len(chapters[i - 3]) == len(heading_level):\n",
        "                # Если предыдущая глава на том же уровне, объединяем с ней\n",
        "                prev_heading_title = chapters[i - 2].strip()\n",
        "                prev_chapter_content = chapters[i - 1].strip()\n",
        "                prev_heading_title += f\" + {heading_title}\"\n",
        "                prev_chapter_content += f\"\\n{chapter_content}\"\n",
        "                chapters[i - 1] = prev_chapter_content\n",
        "                chapters[i + 1] = ''\n",
        "                chapters[i + 2] = ''\n",
        "            elif i + 3 < len(chapters) and len(chapters[i + 3]) > len(heading_level):\n",
        "                # Если следующая глава на более глубоком уровне, объединяем с ней\n",
        "                next_heading_level = chapters[i + 3]\n",
        "                next_heading_title = chapters[i + 4].strip()\n",
        "                next_chapter_content = chapters[i + 5].strip()\n",
        "\n",
        "                combined_content = f\"{chapter_content}\\n{next_chapter_content}\"\n",
        "                tokens_count_combined = count_tokens(combined_content)\n",
        "\n",
        "                if tokens_count_combined < max_tokens:\n",
        "                    chapter_content = combined_content\n",
        "                    chapters[i + 5] = chapter_content\n",
        "                else:\n",
        "                    for idx in range(i + 5, len(chapters), 3):\n",
        "                        if len(chapters[idx]) == len(next_heading_level):\n",
        "                            chapters[idx] += f\"\\n{chapter_content}\"\n",
        "\n",
        "        # Добавляем обработанную главу в результат\n",
        "        result.append(f\"{heading_level} {heading_title}\\n{chapter_content}\\n\")\n",
        "\n",
        "    # Возвращаем объединенный текст из всех обработанных частей\n",
        "    return ''.join(result)\n",
        "\n",
        "\n",
        "# Удаляет пустые строки из текста.\n",
        "def remove_empty_lines(text):\n",
        "    \"\"\"\n",
        "    Удаляет пустые строки из текста.\n",
        "\n",
        "    Аргументы:\n",
        "    text (str): Исходный текст, из которого нужно удалить пустые строки.\n",
        "\n",
        "    Возвращает:\n",
        "    str: Текст без пустых строк.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n***** Удаление пустых строк *****\\n\")\n",
        "\n",
        "    # Разбиваем текст на строки\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Фильтруем строки, оставляя только непустые\n",
        "    non_empty_lines = [line for line in lines if line.strip()]\n",
        "\n",
        "    # Объединяем непустые строки обратно в текст с разделителем '\\n'\n",
        "    return '\\n'.join(non_empty_lines)\n",
        "\n",
        "\n",
        "# Объединяет смежные главы в Markdown тексте, если они удовлетворяют условиям по количеству токенов.\n",
        "def merge_adjacent_chapters(markdown_text, min_tokens=256, max_tokens=1024):\n",
        "    \"\"\"\n",
        "    Объединяет смежные главы в Markdown тексте, если они удовлетворяют условиям по количеству токенов.\n",
        "\n",
        "    Аргументы:\n",
        "    markdown_text (str): Исходный текст в формате Markdown.\n",
        "\n",
        "    Возвращает:\n",
        "    str: Текст с объединенными смежными главами.\n",
        "    \"\"\"\n",
        "    # Регулярное выражение для нахождения заголовков (от уровня 1 до уровня 6)\n",
        "    heading_pattern = re.compile(r'^(#{1,6})\\s(.+)', re.MULTILINE)\n",
        "\n",
        "    # Разделяем текст на главы, сохраняя заголовки и контент\n",
        "    chapters = heading_pattern.split(markdown_text)[1:]\n",
        "    merged_chapters = []\n",
        "\n",
        "    i = 0\n",
        "    while i < len(chapters):\n",
        "        # Текущий заголовок и его содержимое\n",
        "        heading_level = chapters[i]\n",
        "        heading_title = chapters[i + 1].strip()\n",
        "        chapter_content = chapters[i + 2].strip()\n",
        "\n",
        "        # Подсчитываем количество токенов в текущей главе\n",
        "        current_tokens_count = count_tokens(chapter_content)\n",
        "\n",
        "        # Проверяем смежные главы и объединяем их, если они удовлетворяют условиям\n",
        "        while (i + 3 < len(chapters) and\n",
        "               len(chapters[i + 3]) == len(heading_level) and\n",
        "               current_tokens_count < max_tokens):\n",
        "\n",
        "            next_heading_title = chapters[i + 4].strip()\n",
        "            next_chapter_content = chapters[i + 5].strip()\n",
        "            next_tokens_count = count_tokens(next_chapter_content)\n",
        "\n",
        "            # Прекращаем объединение, если следующая глава слишком большая\n",
        "            if next_tokens_count > min_tokens:\n",
        "                break\n",
        "\n",
        "            combined_tokens_count = current_tokens_count + next_tokens_count\n",
        "            # Прекращаем объединение, если суммарное количество токенов превышает 1024\n",
        "            if combined_tokens_count > max_tokens:\n",
        "                break\n",
        "\n",
        "            # Удаляем повторяющуюся информацию из заголовков\n",
        "            common_prefix = ''\n",
        "            min_length = min(len(heading_title), len(next_heading_title))\n",
        "            for j in range(min_length):\n",
        "                if heading_title[j] == next_heading_title[j]:\n",
        "                    common_prefix += heading_title[j]\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            # Обновляем заголовок текущей главы, добавляя заголовок следующей\n",
        "            heading_title = heading_title[len(common_prefix):] + ' + ' + next_heading_title[len(common_prefix):]\n",
        "            # Объединяем содержимое текущей и следующей глав\n",
        "            chapter_content += f\"\\n{next_chapter_content}\"\n",
        "            current_tokens_count = combined_tokens_count\n",
        "\n",
        "            # Переходим к следующей главе\n",
        "            i += 3\n",
        "\n",
        "        # Добавляем объединенную (или оставшуюся неизменной) главу в результат\n",
        "        merged_chapters.append(f\"{heading_level} {heading_title}\\n{chapter_content}\")\n",
        "        i += 3\n",
        "\n",
        "    # Объединяем все главы в один текст и возвращаем результат\n",
        "    return '\\n'.join(merged_chapters)\n",
        "\n",
        "\n",
        "def print_single_child_sections(markdown_text):\n",
        "\n",
        "    # Регулярное выражение для поиска заголовков\n",
        "    header_pattern = re.compile(r'^(#+) (.+)$', re.MULTILINE)\n",
        "\n",
        "    # Найти все заголовки\n",
        "    headers = [(m.start(), m.groups()) for m in header_pattern.finditer(markdown_text)]\n",
        "\n",
        "    def get_level(header):\n",
        "        return len(header[1][0])\n",
        "\n",
        "    def get_children(start_index, level):\n",
        "        children = []\n",
        "        i = start_index + 1\n",
        "        while i < len(headers):\n",
        "            if get_level(headers[i]) == level + 1:\n",
        "                children.append(i)\n",
        "            elif get_level(headers[i]) <= level:\n",
        "                break\n",
        "            i += 1\n",
        "        return children\n",
        "\n",
        "    # Проверить каждый заголовок\n",
        "    for i in range(len(headers)):\n",
        "        start_index = i\n",
        "        level = get_level(headers[start_index])\n",
        "        children = get_children(start_index, level)\n",
        "        if len(children) == 1:\n",
        "            _, title = headers[start_index][1]\n",
        "            print(title)\n",
        "\n",
        "# Объединяет разделы Markdown текста, у которых есть единственный дочерний элемент,\n",
        "# в один раздел, если их общий размер не превышает заданное количество токенов.\n",
        "# Уровень остается родительским, а заголовок дочерним.\n",
        "def combine_single_child_sections(markdown_text, max_tokens=1024):\n",
        "    \"\"\"\n",
        "    Объединяет разделы Markdown текста, у которых есть единственный дочерний элемент,\n",
        "    в один раздел, если их общий размер не превышает заданное количество токенов.\n",
        "    Уровень остается родительским, а заголовок дочерним.\n",
        "\n",
        "    Аргументы:\n",
        "    markdown_text (str): Исходный текст в формате Markdown.\n",
        "    max_tokens (int): Максимальное количество токенов для объединенного раздела.\n",
        "\n",
        "    Возвращает:\n",
        "    str: Текст с объединенными разделами.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"***** Объединяет разделы у которых есть единственный дочерний элемент *****\\n\")\n",
        "\n",
        "    # Регулярное выражение для поиска заголовков (например, # Заголовок, ## Подзаголовок и т.д.)\n",
        "    header_pattern = re.compile(r'^(#+) (.+)$', re.MULTILINE)\n",
        "\n",
        "    # Найти все заголовки в тексте и сохранить их позиции и уровни\n",
        "    headers = [(m.start(), m.groups()) for m in header_pattern.finditer(markdown_text)]\n",
        "\n",
        "    def get_level(header):\n",
        "        \"\"\"\n",
        "        Определяет уровень заголовка (количество символов #).\n",
        "\n",
        "        Аргументы:\n",
        "        header (tuple): Кортеж, содержащий информацию о заголовке.\n",
        "\n",
        "        Возвращает:\n",
        "        int: Уровень заголовка.\n",
        "        \"\"\"\n",
        "        return len(header[1][0])\n",
        "\n",
        "    def get_children(start_index, level):\n",
        "        \"\"\"\n",
        "        Находит все дочерние заголовки для заданного заголовка.\n",
        "\n",
        "        Аргументы:\n",
        "        start_index (int): Индекс начального заголовка в списке заголовков.\n",
        "        level (int): Уровень начального заголовка.\n",
        "\n",
        "        Возвращает:\n",
        "        list: Список индексов дочерних заголовков.\n",
        "        \"\"\"\n",
        "        children = []\n",
        "        i = start_index + 1\n",
        "        while i < len(headers):\n",
        "            if get_level(headers[i]) == level + 1:\n",
        "                children.append(i)\n",
        "            elif get_level(headers[i]) <= level:\n",
        "                break\n",
        "            i += 1\n",
        "        return children\n",
        "\n",
        "    # Список для хранения результирующего текста\n",
        "    result = []\n",
        "    i = 0\n",
        "\n",
        "    while i < len(headers):\n",
        "        start_index = i\n",
        "        level = get_level(headers[start_index])\n",
        "        children = get_children(start_index, level)\n",
        "\n",
        "        if len(children) == 1:\n",
        "            # Если у заголовка есть ровно один дочерний заголовок\n",
        "            child_index = children[0]\n",
        "            parent_start, (parent_hashes, parent_title) = headers[start_index]\n",
        "            child_start, (child_hashes, child_title) = headers[child_index]\n",
        "            parent_end = headers[start_index + 1][0] if start_index + 1 < len(headers) else len(markdown_text)\n",
        "            child_end = headers[child_index + 1][0] if child_index + 1 < len(headers) else len(markdown_text)\n",
        "\n",
        "            # Объединенный заголовок (родительский заголовок заменяется заголовком дочернего)\n",
        "            combined_title = f\"{parent_hashes} {child_title}\"\n",
        "\n",
        "            # Объединенное содержимое (контент родительского и дочернего заголовков)\n",
        "            parent_content = markdown_text[parent_start + len(parent_hashes) + 1 + len(parent_title):child_start].strip()\n",
        "            child_content = markdown_text[child_start + len(child_hashes) + 1 + len(child_title):child_end].strip()\n",
        "            combined_content = (parent_content + \"\\n\\n\" + child_content).strip()\n",
        "\n",
        "            count_parent = count_tokens(parent_content)\n",
        "            count_child  = count_tokens(child_content)\n",
        "\n",
        "            if count_parent>(max_tokens/2) and count_child>(max_tokens/2):\n",
        "                current_start = headers[start_index][0]\n",
        "                current_end = headers[start_index + 1][0] if start_index + 1 < len(headers) else len(markdown_text)\n",
        "                result.append(markdown_text[current_start:current_end].strip())\n",
        "                i += 1\n",
        "            else:\n",
        "                combined_content_title = (combined_title + \"\\n\" + parent_content + \"\\n\" + child_content).strip()\n",
        "                result.append(combined_content_title)\n",
        "                print(f\"Обработан раздел - {combined_title}. ({count_tokens(parent_content)} + {count_tokens(child_content)}) {count_tokens(combined_content_title)} токенов\")\n",
        "                # # Проверка и разбиение содержания, если оно превышает max_tokens\n",
        "                # split_contents = split_and_extend_long_chapters(combined_content)\n",
        "                # for part in split_contents:\n",
        "                #     result.append(f\"{combined_title}\\n{part}\")\n",
        "\n",
        "                # Пропускаем дочерний раздел, так как он уже объединен с родительским\n",
        "                i = child_index + 1\n",
        "        else:\n",
        "            # Если у заголовка нет дочерних элементов или их больше одного\n",
        "            # Добавляем текущий заголовок и его содержимое в результат\n",
        "            current_start = headers[start_index][0]\n",
        "            current_end = headers[start_index + 1][0] if start_index + 1 < len(headers) else len(markdown_text)\n",
        "            result.append(markdown_text[current_start:current_end].strip())\n",
        "            i += 1\n",
        "\n",
        "    # Объединяем все части результата в одну строку\n",
        "    return '\\n\\n'.join(result)\n",
        "\n",
        "# Разделяет длинные главы в markdown тексте на части, если они превышают максимальное количество токенов.\n",
        "# Каждая часть получает новый заголовок, указывающий, что это часть длинной главы.\n",
        "def split_and_extend_long_chapters_(markdown_text, max_tokens=1024):\n",
        "    \"\"\"\n",
        "    Разделяет длинные главы в markdown тексте на части, если они превышают максимальное количество токенов.\n",
        "    Каждая часть получает новый заголовок, указывающий, что это часть длинной главы.\n",
        "\n",
        "    Аргументы:\n",
        "    markdown_text (str): Текст в формате markdown для обработки.\n",
        "    max_tokens (int): Максимальное количество токенов в каждой части главы.\n",
        "\n",
        "    Возвращает:\n",
        "    str: Обработанный markdown текст с разделенными главами.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"***** Разделяет длинные главы в тексте на части, если они превышают максимальное количество токенов. *****\\n\")\n",
        "\n",
        "    # Регулярное выражение для поиска заголовков в markdown тексте\n",
        "    heading_pattern = re.compile(r'^(#{1,6})\\s(.+)', re.MULTILINE)\n",
        "\n",
        "    # Разделение текста на главы по заголовкам\n",
        "    chapters = heading_pattern.split(markdown_text)[1:]\n",
        "\n",
        "    # Инициализация списка для хранения обработанных частей текста\n",
        "    result = []\n",
        "\n",
        "    # Список разделителей для предпочтительного деления текста\n",
        "    delimiters = [\"\\n\\n\", \"\\n\"]\n",
        "\n",
        "    # Проход по каждой главе\n",
        "    for i in range(0, len(chapters), 3):\n",
        "        heading_level = chapters[i]  # Уровень заголовка (например, \"##\")\n",
        "        heading_title = chapters[i + 1].strip()  # Заголовок главы\n",
        "        chapter_content = chapters[i + 2]  # Содержимое главы\n",
        "        heading_level_title = f\"{heading_level} {heading_title}\"\n",
        "\n",
        "        # Подсчет количества токенов в содержимом главы\n",
        "        tokens_count = count_tokens(chapter_content)\n",
        "        if tokens_count > max_tokens:\n",
        "            print(f\"{heading_level_title} - {tokens_count} токенов\")\n",
        "            # Если количество токенов превышает максимальное, делим содержимое главы на части\n",
        "            parts = split_text_by_delimiters_(chapter_content, max_tokens, delimiters)\n",
        "\n",
        "            # Создаем заголовки для каждой части и добавляем их в результат\n",
        "            for idx, part in enumerate(parts):\n",
        "                part_title = f\"{heading_title} - part {idx + 1}\"\n",
        "                result.append(f\"{heading_level} {part_title}\\n{part}\\n\")\n",
        "                print(f\"     {heading_level} {part_title} - {count_tokens(part)} токенов\")\n",
        "        else:\n",
        "            # Если количество токенов не превышает максимальное, добавляем главу целиком\n",
        "            result.append(f\"{heading_level}{heading_title}{chapter_content}\")\n",
        "\n",
        "    # Возвращаем объединенный текст из всех обработанных частей\n",
        "    return ''.join(result)\n",
        "\n",
        "# Функция переносит содержимое родительских разделов в дочерние разделы, если длина содержимого родительского раздела\n",
        "# в токенах меньше или равна указанному минимальному количеству токенов (min_tokens).\n",
        "def parent_to_child(markdown_text, min_tokens=256):\n",
        "    \"\"\"\n",
        "    Функция переносит содержимое родительских разделов в дочерние разделы, если длина содержимого родительского раздела\n",
        "    в токенах меньше или равна указанному минимальному количеству токенов (min_tokens). Функция работает только с\n",
        "    заголовками второго уровня и ниже. Заголовок родительского раздела остается.\n",
        "\n",
        "    Аргументы:\n",
        "    markdown_text (str): Входной текст в формате Markdown, содержащий разделы с заголовками.\n",
        "    min_tokens (int): Минимальное количество токенов, при котором содержимое родительского раздела будет скопировано\n",
        "                      в дочерние разделы.\n",
        "    max_tokens (int): Максимальное количество токенов в одном разделе после объединения содержимого.\n",
        "\n",
        "    Возвращает:\n",
        "    str: Обновленный текст в формате Markdown с объединенным содержимым разделов.\n",
        "    \"\"\"\n",
        "\n",
        "    # Регулярное выражение для поиска заголовков\n",
        "    header_pattern = re.compile(r'^(#+) (.+)$', re.MULTILINE)\n",
        "\n",
        "    # Найти все заголовки\n",
        "    headers = [(m.start(), m.groups()) for m in header_pattern.finditer(markdown_text)]\n",
        "\n",
        "    def get_level(header):\n",
        "        \"\"\"Возвращает уровень заголовка на основе количества символов #\"\"\"\n",
        "        return len(header[1][0])\n",
        "\n",
        "    def get_children(start_index, level):\n",
        "        \"\"\"Возвращает индексы всех дочерних разделов для данного заголовка\"\"\"\n",
        "        children = []\n",
        "        i = start_index + 1\n",
        "        while i < len(headers):\n",
        "            if get_level(headers[i]) == level + 1:\n",
        "                children.append(i)\n",
        "            elif get_level(headers[i]) <= level:\n",
        "                break\n",
        "            i += 1\n",
        "        return children\n",
        "\n",
        "    # Список для хранения результирующего текста\n",
        "    result = []\n",
        "\n",
        "    i = 0\n",
        "    while i < len(headers):\n",
        "        start_index = i\n",
        "        level = get_level(headers[start_index])\n",
        "\n",
        "        # Пропускаем заголовки первого уровня\n",
        "        if level == 1:\n",
        "            current_start = headers[start_index][0]\n",
        "            current_end = headers[start_index + 1][0] if start_index + 1 < len(headers) else len(markdown_text)\n",
        "            result.append(markdown_text[current_start:current_end].strip())\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        children = get_children(start_index, level)\n",
        "\n",
        "        if len(children) > 1:\n",
        "            parent_index = start_index\n",
        "            parent_start, (parent_hashes, parent_title) = headers[parent_index]\n",
        "            parent_end = headers[parent_index + 1][0] if parent_index + 1 < len(headers) else len(markdown_text)\n",
        "            parent_content = markdown_text[parent_start + len(parent_hashes) + 1 + len(parent_title):parent_end].strip()\n",
        "\n",
        "            # Проверка длины родительского раздела в токенах\n",
        "            parent_tokens = count_tokens(parent_content)\n",
        "            if parent_tokens <= min_tokens:\n",
        "                # Сохраняем содержимое дочерних разделов\n",
        "                child_contents = []\n",
        "                for child_index in children:\n",
        "                    child_start, (child_hashes, child_title) = headers[child_index]\n",
        "                    child_end = headers[child_index + 1][0] if child_index + 1 < len(headers) else len(markdown_text)\n",
        "                    child_content = markdown_text[child_start + len(child_hashes) + 1 + len(child_title):child_end].strip()\n",
        "                    child_contents.append(child_content)\n",
        "\n",
        "                # Добавляем заголовок родительского раздела\n",
        "                result.append(f\"{parent_hashes} {parent_title}\")\n",
        "\n",
        "                # Копируем содержимое родительского раздела в дочерние разделы\n",
        "                for child_index, child_content in zip(children, child_contents):\n",
        "                    child_start, (child_hashes, child_title) = headers[child_index]\n",
        "                    # Объединенный заголовок и содержимое\n",
        "                    combined_title = f\"{child_hashes} {child_title}\"\n",
        "                    combined_content = (parent_content + \"\\n\" + child_content).strip()\n",
        "                    combined_content_title = (combined_title + \"\\n\" +parent_content + \"\\n\" + child_content).strip()\n",
        "                    result.append(combined_content_title)\n",
        "\n",
        "                    # split_contents = split_and_extend_long_chapters(f\"{combined_title}\\n{combined_content}\", max_tokens)\n",
        "                    # for part in split_contents:\n",
        "                    #     result.append(part)\n",
        "                # Пропускаем все дочерние разделы\n",
        "                i = children[-1] + 1\n",
        "            else:\n",
        "                # Добавляем текущий заголовок и его содержимое без изменений\n",
        "                current_start = headers[start_index][0]\n",
        "                current_end = headers[start_index + 1][0] if start_index + 1 < len(headers) else len(markdown_text)\n",
        "                result.append(markdown_text[current_start:current_end].strip())\n",
        "                i += 1\n",
        "        else:\n",
        "            # Добавляем текущий заголовок и его содержимое без изменений\n",
        "            current_start = headers[start_index][0]\n",
        "            current_end = headers[start_index + 1][0] if start_index + 1 < len(headers) else len(markdown_text)\n",
        "            result.append(markdown_text[current_start:current_end].strip())\n",
        "            i += 1\n",
        "\n",
        "    # Объединяем результат в одну строку\n",
        "    return '\\n\\n'.join(result)\n",
        "\n",
        "# Функция проверяет уровни заголовков в тексте Markdown, чтобы они следовали иерархически правильной структуре.\n",
        "def check_markdown_headings(markdown_text):\n",
        "    # Регулярное выражение для поиска заголовков\n",
        "    heading_pattern = re.compile(r'^(#{1,6})\\s', re.MULTILINE)\n",
        "    headings = heading_pattern.findall(markdown_text)\n",
        "\n",
        "    # Проверка очередности заголовков\n",
        "    expected_level = 0\n",
        "\n",
        "    for heading in headings:\n",
        "        current_level = len(heading)\n",
        "        if current_level > expected_level + 1:\n",
        "            print(f\"Ошибка: Заголовок {heading} на уровне {current_level} идет после уровня {expected_level}.\")\n",
        "            return False\n",
        "        expected_level = current_level\n",
        "\n",
        "    print(\"Все заголовки в правильной последовательности.\")\n",
        "    return True\n",
        "\n",
        "def fix_markdown_headings(markdown_text):\n",
        "    \"\"\"\n",
        "    Функция проверяет и исправляет уровни заголовков в тексте Markdown, чтобы они следовали иерархически правильной структуре.\n",
        "\n",
        "    Аргументы:\n",
        "    markdown_text (str): Входной текст в формате Markdown, содержащий заголовки.\n",
        "\n",
        "    Возвращает:\n",
        "    str: Текст в формате Markdown с исправленными уровнями заголовков.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"***** Обработчик правильности уровней вложенности *****\\n\")\n",
        "\n",
        "    # Регулярное выражение для поиска заголовков\n",
        "    heading_pattern = re.compile(r'^(#{1,6})\\s(.+)', re.MULTILINE)\n",
        "\n",
        "    # Найти все заголовки\n",
        "    headings = heading_pattern.findall(markdown_text)\n",
        "\n",
        "    # Инициализация переменной для хранения исправленного текста\n",
        "    fixed_text = markdown_text\n",
        "\n",
        "    # Инициализация ожидаемого уровня заголовка\n",
        "    expected_level = 0\n",
        "\n",
        "    def replace_heading(match):\n",
        "        \"\"\"\n",
        "        Вложенная функция для замены заголовков на правильный уровень при необходимости.\n",
        "\n",
        "        Аргументы:\n",
        "        match (re.Match): Объект совпадения, найденный регулярным выражением.\n",
        "\n",
        "        Возвращает:\n",
        "        str: Исправленный или оригинальный заголовок.\n",
        "        \"\"\"\n",
        "        nonlocal expected_level\n",
        "        current_level = len(match.group(1))  # Текущий уровень заголовка\n",
        "        heading_text = match.group(2)        # Текст заголовка\n",
        "\n",
        "        # print(f\"Обработка заголовка: '{heading_text}' с уровнем {current_level}\")\n",
        "\n",
        "        if current_level > expected_level + 1:\n",
        "            # Если уровень заголовка больше ожидаемого более чем на 1, понижаем его\n",
        "            new_level = expected_level + 1\n",
        "            corrected_heading = '#' * new_level\n",
        "            print(f\"Исправление: Заголовок {heading_text}\")\n",
        "            print(f\"                                       с уровня {current_level} заменен на уровень {new_level}.\")\n",
        "            return corrected_heading + ' ' + heading_text\n",
        "        else:\n",
        "            # Обновляем ожидаемый уровень заголовка на текущий уровень\n",
        "            expected_level = current_level\n",
        "            return match.group(0)  # Возвращаем оригинальный заголовок\n",
        "\n",
        "    # Замена заголовков в тексте на исправленные\n",
        "    fixed_text = heading_pattern.sub(replace_heading, fixed_text)\n",
        "\n",
        "    return fixed_text\n",
        "\n",
        "# Чистка текста и исправление разбиения.\n",
        "def headings_and_empty_lines(markdown_text):\n",
        "    text_out = markdown_text\n",
        "    text_out = text_out.replace('\\ufeff', '')            # Удаление BOM\n",
        "    text_out = remove_empty_lines(text_out)              # Удаление пустых строк.\n",
        "    text_out = fix_markdown_headings(text_out) # Проверка иерархии\n",
        "    return text_out\n",
        "\n",
        "# Добавляет к заголовку число токенов.\n",
        "def add_token_counts_to_headings(markdown_text):\n",
        "\n",
        "    print(\"***** Добавление количества токенов к заголовку *****\\n\")\n",
        "\n",
        "    # Регулярное выражение для поиска заголовков (например, # Заголовок, ## Подзаголовок и т.д.)\n",
        "    header_pattern = re.compile(r'^(#+) (.+)$', re.MULTILINE)\n",
        "    # Найти все заголовки в тексте и сохранить их позиции и уровни\n",
        "    headers = [(m.start(), m.groups()) for m in header_pattern.finditer(markdown_text)]\n",
        "    # Список для хранения результирующего текста\n",
        "    result = []\n",
        "    i = 0\n",
        "    while i < len(headers):\n",
        "        start_index = i\n",
        "        current_start = headers[start_index][0]\n",
        "        current_end   = headers[start_index + 1][0] if start_index + 1 < len(headers) else len(markdown_text)\n",
        "        current_start, (current_hashes, current_title) = headers[start_index]\n",
        "        current_content = markdown_text[current_start + len(current_hashes) + 1 + len(current_title):current_end].strip()\n",
        "        a = markdown_text[current_start:current_end].strip()\n",
        "        current_tokens = count_tokens(markdown_text[current_start:current_end].strip())\n",
        "        current_combined = f\"{current_hashes} {current_title} ({current_tokens} токенов)\\n{current_content}\"\n",
        "        result.append(current_combined)\n",
        "        i += 1\n",
        "    return '\\n'.join(result)\n",
        "\n",
        "# Добавляет номер к заголовку.\n",
        "def renumber_headings(markdown_text):\n",
        "    \"\"\"\n",
        "    Функция для нумерации заголовков в тексте Markdown.\n",
        "\n",
        "    Аргументы:\n",
        "    markdown_text (str): Входной текст в формате Markdown, содержащий заголовки.\n",
        "\n",
        "    Возвращает:\n",
        "    str: Текст в формате Markdown с пронумерованными заголовками.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n***** Нумерации заголовков. *****\\n\")\n",
        "\n",
        "    # Регулярное выражение для поиска заголовков\n",
        "    header_pattern = re.compile(r'^(#{1,6})\\s(.+)', re.MULTILINE)\n",
        "\n",
        "    # Найти все заголовки\n",
        "    headers = list(header_pattern.finditer(markdown_text))\n",
        "\n",
        "    # Словарь для отслеживания текущей нумерации на каждом уровне\n",
        "    heading_numbers = {}\n",
        "\n",
        "    def get_heading_number(level):\n",
        "        \"\"\"\n",
        "        Возвращает номер заголовка на основе текущей нумерации.\n",
        "\n",
        "        Аргументы:\n",
        "        level (int): Уровень заголовка (1 для #, 2 для ## и т.д.)\n",
        "\n",
        "        Возвращает:\n",
        "        str: Пронумерованный заголовок для данного уровня.\n",
        "        \"\"\"\n",
        "        if level not in heading_numbers:\n",
        "            heading_numbers[level] = 0\n",
        "        heading_numbers[level] += 1\n",
        "\n",
        "        # Сброс нумерации на более глубоких уровнях\n",
        "        for deeper_level in range(level + 1, 7):\n",
        "            if deeper_level in heading_numbers:\n",
        "                heading_numbers[deeper_level] = 0\n",
        "\n",
        "        # Создание номера заголовка, например, 1.1.2\n",
        "        return '.'.join(str(heading_numbers[i]) for i in range(1, level + 1))\n",
        "\n",
        "    # Инициализация переменной для хранения результирующего текста\n",
        "    result = []\n",
        "    last_end = 0\n",
        "\n",
        "    for header in headers:\n",
        "        level = len(header.group(1))\n",
        "        title = header.group(2)\n",
        "        start = header.start()\n",
        "        end = header.end()\n",
        "\n",
        "        # Добавляем текст до текущего заголовка\n",
        "        result.append(markdown_text[last_end:start])\n",
        "\n",
        "        # Получаем пронумерованный заголовок\n",
        "        heading_number = get_heading_number(level)\n",
        "        updated_header = f\"{'#' * level} {heading_number} {title}\"\n",
        "\n",
        "        # Добавляем пронумерованный заголовок в результат\n",
        "        result.append(updated_header)\n",
        "        last_end = end\n",
        "\n",
        "    # Добавляем оставшийся текст после последнего заголовка\n",
        "    result.append(markdown_text[last_end:])\n",
        "\n",
        "    return ''.join(result)\n",
        "\n",
        "# Копирование заголовка в текст главы.\n",
        "def copy_headings_into_content(markdown_text):\n",
        "    \"\"\"\n",
        "    Функция копирует заголовки в текст соответствующих глав.\n",
        "\n",
        "    Аргументы:\n",
        "    markdown_text (str): Входной текст в формате Markdown, содержащий заголовки.\n",
        "\n",
        "    Возвращает:\n",
        "    str: Текст в формате Markdown с заголовками, скопированными в текст соответствующих глав.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n***** Копирование заголовков в текст соответствующих глав *****\\n\")\n",
        "\n",
        "    # Регулярное выражение для поиска заголовков\n",
        "    header_pattern = re.compile(r'^(#{1,6})\\s(.+)', re.MULTILINE)\n",
        "\n",
        "    # Найти все заголовки\n",
        "    headers = list(header_pattern.finditer(markdown_text))\n",
        "\n",
        "    # Инициализация переменной для хранения результирующего текста\n",
        "    result = []\n",
        "    last_end = 0\n",
        "\n",
        "    for i, header in enumerate(headers):\n",
        "        level = len(header.group(1))\n",
        "        title = header.group(2)\n",
        "        start = header.start()\n",
        "        end = header.end()\n",
        "\n",
        "        # Добавляем текст до текущего заголовка\n",
        "        result.append(markdown_text[last_end:start])\n",
        "\n",
        "        # Добавляем заголовок\n",
        "        result.append(markdown_text[start:end])\n",
        "\n",
        "        # Определяем конец текущей главы (начало следующей главы или конец текста)\n",
        "        next_start = headers[i + 1].start() if i + 1 < len(headers) else len(markdown_text)\n",
        "\n",
        "        # Получаем содержимое текущей главы\n",
        "        chapter_content = markdown_text[end:next_start].strip()\n",
        "\n",
        "        # Создаем обновленный контент с заголовком\n",
        "        updated_content = f\"**{title}**\\n{chapter_content}\"\n",
        "\n",
        "        # Добавляем обновленный контент в результат\n",
        "        result.append(\"\\n\" + updated_content + \"\\n\")\n",
        "\n",
        "        last_end = next_start\n",
        "\n",
        "    # Добавляем оставшийся текст после последнего заголовка\n",
        "    result.append(markdown_text[last_end:])\n",
        "\n",
        "    return ''.join(result)\n",
        "\n",
        "# Приведение всех списков к единому виду нумерованных списков.\n",
        "def convert_lists_to_ordered(markdown_text):\n",
        "    \"\"\"\n",
        "    Функция преобразует все виды списков в тексте глав в нумерованный формат Markdown с последовательной нумерацией.\n",
        "\n",
        "    Аргументы:\n",
        "    markdown_text (str): Входной текст в формате Markdown, содержащий заголовки и списки.\n",
        "\n",
        "    Возвращает:\n",
        "    str: Текст в формате Markdown с преобразованными списками.\n",
        "    \"\"\"\n",
        "    print(\"\\n***** Приведение всех списков к единому виду нумерованных списков. *****\\n\")\n",
        "\n",
        "    # Регулярное выражение для поиска заголовков\n",
        "    header_pattern = re.compile(r'^(#{1,6})\\s(.+)', re.MULTILINE)\n",
        "\n",
        "    # Найти все заголовки\n",
        "    headers = list(header_pattern.finditer(markdown_text))\n",
        "\n",
        "    # Инициализация переменной для хранения результирующего текста\n",
        "    result = []\n",
        "    last_end = 0\n",
        "\n",
        "    def convert_to_ordered_list(text):\n",
        "        \"\"\"\n",
        "        Преобразует все виды списков в нумерованный формат с последовательной нумерацией.\n",
        "\n",
        "        Аргументы:\n",
        "        text (str): Входной текст, содержащий списки.\n",
        "\n",
        "        Возвращает:\n",
        "        str: Текст с преобразованными списками.\n",
        "        \"\"\"\n",
        "        lines = text.split('\\n')\n",
        "        ordered_list = []\n",
        "        list_counter = 0\n",
        "        inside_list = False\n",
        "\n",
        "        for line in lines:\n",
        "            # Определяем, является ли текущая строка элементом списка\n",
        "            if re.match(r'^\\s*[-*]\\s+', line):\n",
        "                list_counter += 1\n",
        "                inside_list = True\n",
        "                # Заменяем маркер списка на нумерованный формат\n",
        "                ordered_line = re.sub(r'^(\\s*)[-*]\\s+', '  ' + str(list_counter) + '. ', line)\n",
        "                ordered_list.append(ordered_line)\n",
        "            elif re.match(r'^\\s*\\d+\\.\\s+', line):\n",
        "                if not inside_list:\n",
        "                    list_counter = 1\n",
        "                else:\n",
        "                    list_counter += 1\n",
        "                inside_list = True\n",
        "                # Заменяем существующий нумерованный маркер на правильный номер\n",
        "                ordered_line = re.sub(r'^\\s*\\d+\\.\\s+', '  ' + str(list_counter) + '. ', line)\n",
        "                ordered_list.append(ordered_line)\n",
        "            else:\n",
        "                if inside_list:\n",
        "                    inside_list = False\n",
        "                    list_counter = 0\n",
        "                ordered_list.append(line)\n",
        "\n",
        "        return '\\n'.join(ordered_list)\n",
        "\n",
        "    for i, header in enumerate(headers):\n",
        "        start = header.start()\n",
        "        end = header.end()\n",
        "\n",
        "        # Добавляем текст до текущего заголовка\n",
        "        result.append(markdown_text[last_end:start])\n",
        "\n",
        "        # Добавляем заголовок\n",
        "        result.append(markdown_text[start:end])\n",
        "\n",
        "        # Определяем конец текущей главы (начало следующей главы или конец текста)\n",
        "        next_start = headers[i + 1].start() if i + 1 < len(headers) else len(markdown_text)\n",
        "\n",
        "        # Получаем содержимое текущей главы\n",
        "        chapter_content = markdown_text[end:next_start].strip()\n",
        "\n",
        "        # Преобразуем списки в нумерованные\n",
        "        updated_content = convert_to_ordered_list(chapter_content)\n",
        "\n",
        "        # Добавляем обновленный контент в результат\n",
        "        result.append(\"\\n\" + updated_content + \"\\n\")\n",
        "\n",
        "        last_end = next_start\n",
        "\n",
        "    # Добавляем оставшийся текст после последнего заголовка\n",
        "    result.append(markdown_text[last_end:])\n",
        "\n",
        "    return ''.join(result)\n",
        "\n",
        "\n",
        "def create_toc(markdown_text):\n",
        "    \"\"\"\n",
        "    Функция создает содержание (Table of Contents, TOC) с нумерацией по заголовкам первого уровня из текста в формате Markdown.\n",
        "\n",
        "    Аргументы:\n",
        "    markdown_text (str): Входной текст в формате Markdown, содержащий заголовки и содержимое.\n",
        "\n",
        "    Возвращает:\n",
        "    str: Текст содержания с нумерованными заголовками первого уровня.\n",
        "    \"\"\"\n",
        "    print(\"\\n***** Создание оглавления. *****\\n\")\n",
        "\n",
        "    # Регулярное выражение для поиска заголовков первого уровня\n",
        "    header_pattern = re.compile(r'^(# )(.+)', re.MULTILINE)\n",
        "\n",
        "    # Найти все заголовки первого уровня\n",
        "    headers = header_pattern.findall(markdown_text)\n",
        "\n",
        "    # Создать содержание с нумерацией\n",
        "    toc = [\"Содержание:\"]\n",
        "    for i, header in enumerate(headers, start=1):\n",
        "        toc.append(f\"{i}. {header[1]}\")\n",
        "\n",
        "    return '\\n'.join(toc)"
      ],
      "metadata": {
        "id": "_ntEjWk1zpKJ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wr9jQm4dvOeR",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Функции стадий деления текста.\n",
        "\n",
        "# Возвращает количество токенов в строке.\n",
        "def num_tokens_from_string(string: str) -> int:\n",
        "    '''Возвращает количество токенов в строке'''\n",
        "    # Выбор кодировщика. `cl100k_base`используется для `gpt-4`, `gpt-3.5-turbo`, `text-embedding-ada-002`\n",
        "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    # Разбивка строки на токены и подсчет из количества.\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "# Функция деления по маркдауну. Используется только на первом уровне деления.\n",
        "def markdown_spliter(text: str, count_headers: int) -> List[Document]:\n",
        "    '''Функция деления по маркдауну. Используется только на первом уровне деления.'''\n",
        "    # Объявляем названия заголовков для метаданных.\n",
        "    headers_to_split_on = [(f\"{'#' * i}\", f\"H{i}\") for i in range(1, count_headers)]\n",
        "    # Создаем разделитель.\n",
        "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "    # Разбиваем на фрагменты.\n",
        "    return markdown_splitter.split_text(text)\n",
        "\n",
        "# Функция рекурсивного деления. В зависимости от того что в нее передается text или docs выполняется деление\n",
        "# первого или второго уровня соответственно.\n",
        "def recursive_characte_text_splitter(text:str,\n",
        "                                     docs:List[Document],\n",
        "                                     chunk_size:int,\n",
        "                                     chunk_overlap:int,\n",
        "                                     length_function: str) -> List[Document]:\n",
        "    '''Функция рекурсивного деления. В зависимости от того что в нее передается text или docs выполняется деление\n",
        "    первого или второго уровня соответственно.'''\n",
        "    # Функция подсчета токенов.\n",
        "    def num_tokens(fragment):\n",
        "        return num_tokens_from_string(fragment)\n",
        "\n",
        "    # Определение какая функция подсчета будет использоваться токенов или символов.\n",
        "    if length_function=='token':\n",
        "        splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=num_tokens)\n",
        "    else:\n",
        "        splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len)\n",
        "\n",
        "    # Выполняем деление в зависимости от стадии первой или второй.\n",
        "    if text:\n",
        "        return  [Document(page_content=chunk, metadata={})\n",
        "                    for chunk in splitter.split_text(text)]\n",
        "    else:\n",
        "        return  [Document(page_content=chunk, metadata=doc.metadata)\n",
        "                    for doc in docs for chunk in splitter.split_text(doc.page_content)]\n",
        "\n",
        "# Функция рекурсивного деления. В зависимости от того что в нее передается text или docs выполняется деление\n",
        "# первого или второго уровня соответственно.\n",
        "def semantic_text_splitter_hf(text:str,\n",
        "                            docs:List[Document],\n",
        "                            semantic_chunk_size_min:int,\n",
        "                            semantic_chunk_size_max:int) -> List[Document]:\n",
        "    '''Функция семантического деления. В зависимости от того что в нее передается text или docs выполняется деление\n",
        "    первого или второго уровня соответственно.'''\n",
        "    tokenizer  = Tokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    splitter   = HuggingFaceTextSplitter(tokenizer, trim_chunks=False)\n",
        "    MIN_TOKENS = semantic_chunk_size_min\n",
        "    MAX_TOKENS = semantic_chunk_size_max\n",
        "\n",
        "    if text:\n",
        "        return  [Document(page_content=chunk, metadata={})\n",
        "                    for chunk in splitter.chunks(text, chunk_capacity=(MIN_TOKENS, MAX_TOKENS))]\n",
        "    else:\n",
        "        return  [Document(page_content=chunk, metadata=doc.metadata)\n",
        "                    for doc in docs for chunk in splitter.chunks(doc.page_content, chunk_capacity=(MIN_TOKENS, MAX_TOKENS))]\n",
        "\n",
        "# Функция деления на основе \"Semantic Chunking (Greg Kamradt)\" от того что в нее передается\n",
        "# text или docs выполняется деление первого или второго уровня соответственно.\n",
        "def semantic_text_splitter_gk(text, docs, semantic_breakpoint) -> List[Document]:\n",
        "    # Создание эьбеддинга.\n",
        "    embeddings = HuggingFaceBgeEmbeddings(\n",
        "        model_name=\"BAAI/bge-m3\",\n",
        "        model_kwargs={\"device\": \"cuda\"},\n",
        "        encode_kwargs={\"normalize_embeddings\": True})\n",
        "    # Создание сплитера\n",
        "    text_splitter = SemanticChunker(embeddings, breakpoint_threshold_type=semantic_breakpoint)\n",
        "    # Создание загрузчика\n",
        "    loader = UnstructuredMarkdownLoader('/content/fixed_markdown_11.md')\n",
        "\n",
        "    if text:\n",
        "        return  text_splitter.split_documents(loader.load())\n",
        "    else:\n",
        "        return  text_splitter.split_documents(docs)\n",
        "\n",
        "# Функция деления первого уровня.\n",
        "def first_stage_spliter(type_spliter:str,\n",
        "                        text:str,\n",
        "                        docs:List[Document],\n",
        "                        count_headers:int,\n",
        "                        chunk_size:int,\n",
        "                        chunk_overlap:int,\n",
        "                        length_function: str,\n",
        "                        semantic_chunk_size_min:int,\n",
        "                        semantic_chunk_size_max:int,\n",
        "                        semantic_breakpoint:str) -> List[Document]:\n",
        "    fragments:List[Document] = []\n",
        "    if   type_spliter=='MarkdownHeaderTextSplitter':\n",
        "        fragments = markdown_spliter(text, count_headers)\n",
        "    elif type_spliter=='RecursiveCharacterTextSplitter':\n",
        "        fragments = recursive_characte_text_splitter(text, docs,  chunk_size, chunk_overlap, length_function)\n",
        "    elif type_spliter=='Semantic-Text-Splitter(HuggingFaceTextSplitter)':\n",
        "        fragments = semantic_text_splitter_hf(text, docs,  semantic_chunk_size_min, semantic_chunk_size_max)\n",
        "    elif type_spliter=='Semantic Chunking (Greg Kamradt)':\n",
        "        fragments = semantic_text_splitter_gk(text, docs, semantic_breakpoint)\n",
        "\n",
        "    print(f\"Первая стадия разбивки: {type_spliter}. Число фрагментов: {len(fragments)} шт.\")\n",
        "    return fragments\n",
        "\n",
        "# Функция деления второго уровня.\n",
        "def second_stage_spliter(type_spliter:str,\n",
        "                        docs:List[Document],\n",
        "                        chunk_size:int,\n",
        "                        chunk_overlap:int,\n",
        "                        length_function: str,\n",
        "                        semantic_chunk_size_min:int,\n",
        "                        semantic_chunk_size_max:int,\n",
        "                        semantic_breakpoint:str ) -> List[Document]:\n",
        "\n",
        "    fragments:List[Document] = []\n",
        "    if type_spliter=='RecursiveCharacterTextSplitter':\n",
        "        fragments = recursive_characte_text_splitter('', docs,  chunk_size, chunk_overlap, length_function)\n",
        "    elif type_spliter=='Semantic-Text-Splitter(HuggingFaceTextSplitter)':\n",
        "        fragments = semantic_text_splitter_hf('', docs,  semantic_chunk_size_min, semantic_chunk_size_max)\n",
        "    elif type_spliter == 'None':\n",
        "        fragments = docs\n",
        "    elif type_spliter=='Semantic Chunking (Greg Kamradt)':\n",
        "        fragments = semantic_text_splitter_gk('', docs, semantic_breakpoint)\n",
        "    print(f\"Вторая стадия разбивки: {type_spliter}. Число фрагментов: {len(fragments)} шт.\")\n",
        "    return fragments\n",
        "\n",
        "\n",
        "# Главная функция разбивки текста.\n",
        "def split_text( text,\n",
        "                first_stage_split:str,\n",
        "                second_stage_split:str,\n",
        "                chunk_size:int,\n",
        "                chunk_overlap:int,\n",
        "                length_function:str,\n",
        "                semantic_chunk_size_min:int,\n",
        "                semantic_chunk_size_max:int,\n",
        "                semantic_breakpoint:str):\n",
        "\n",
        "    # Вызов деления первого уровня.\n",
        "    fragments = first_stage_spliter(type_spliter=first_stage_split,\n",
        "                                    text=text,\n",
        "                                    docs=[],\n",
        "                                    count_headers=5,\n",
        "                                    chunk_size=chunk_size,\n",
        "                                    chunk_overlap=chunk_overlap,\n",
        "                                    length_function=length_function,\n",
        "                                    semantic_chunk_size_min=semantic_chunk_size_min,\n",
        "                                    semantic_chunk_size_max=semantic_chunk_size_max,\n",
        "                                    semantic_breakpoint=semantic_breakpoint)\n",
        "\n",
        "    # Вызов деления второго уровня.\n",
        "    docs = second_stage_spliter(type_spliter=second_stage_split,\n",
        "                                docs=fragments,\n",
        "                                chunk_size=chunk_size,\n",
        "                                chunk_overlap=chunk_overlap,\n",
        "                                length_function='token',\n",
        "                                semantic_chunk_size_min=semantic_chunk_size_min,\n",
        "                                semantic_chunk_size_max=semantic_chunk_size_max,\n",
        "                                semantic_breakpoint=semantic_breakpoint)\n",
        "    return docs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83mBnm6RD6LN"
      },
      "source": [
        "## Параметры деления текста"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Предобработка текста\n",
        "link_bd_text = \"https://raw.githubusercontent.com/aypetrunin/copilot/main/%D0%91%D0%B0%D0%B7%D0%B0%20%D0%A3%D0%98%D0%98%20%D0%B4%D0%BB%D1%8F%20copilot%202%20%D1%80%D0%B5%D0%B4%D0%B0%D0%BA%D1%86%D0%B8%D1%8F.md\" # @param {type:\"string\"}\n",
        "token_max = 2550 # @param {type:\"integer\"}\n",
        "token_min = 256 # @param {type:\"integer\"}\n",
        "is_headings_correct = True # @param {type:\"boolean\"}\n",
        "is_extend_chapter = True # @param {type:\"boolean\"}\n",
        "is_split_long_chapters = True # @param {type:\"boolean\"}\n",
        "is_single_child = True # @param {type:\"boolean\"}\n",
        "is_parent_to_child = True # @param {type:\"boolean\"}\n",
        "is_convert_to_ordered_list = True # @param {type:\"boolean\"}\n",
        "is_renumber_headings = True # @param {type:\"boolean\"}\n",
        "is_add_token_counts = True # @param {type:\"boolean\"}\n",
        "is_copy_headings_into_content = True # @param {type:\"boolean\"}\n",
        "is_create_toc = True # @param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "# Функция запуска предобработки.\n",
        "def preprocessing(markdown_text, min_tokens=256, max_tokens=1024, output_filename = 'fixed_markdown.md'):\n",
        "    text_out = markdown_text\n",
        "    save_to_file(text_out, 'fixed_markdown_1.md')\n",
        "\n",
        "    if is_headings_correct:\n",
        "        text_out = headings_and_empty_lines(text_out)\n",
        "        save_to_file(text_out, 'fixed_markdown_2.md')                   # Чистка текста и исправление.\n",
        "\n",
        "    if is_create_toc:\n",
        "        toc = create_toc(text_out)\n",
        "        save_to_file(toc, 'toc.md')\n",
        "\n",
        "    if is_extend_chapter:\n",
        "        text_out = extend_chapter_titles(text_out)                      # Формирование расширенных заголовков.\n",
        "        save_to_file(text_out, 'fixed_markdown_3.md')\n",
        "    if is_split_long_chapters:\n",
        "        text_out = split_and_extend_long_chapters(text_out, max_tokens) # Разбитие длинных разделов.\n",
        "        text_out = remove_empty_lines(text_out)                         # Удаление пустых строк.\n",
        "        save_to_file(text_out, 'fixed_markdown_4.md')\n",
        "    if is_single_child:\n",
        "        text_out = combine_single_child_sections(text_out, max_tokens)  # Объединяет разделы у которых есть единственный дочерний элемент.\n",
        "        save_to_file(text_out, 'fixed_markdown_5.md')\n",
        "    if is_parent_to_child:\n",
        "        text_out = parent_to_child(text_out, min_tokens)                # Копирование родительского раздела в дочернии если род. меньше порога\n",
        "        text_out = remove_empty_lines(text_out)                         # Удаление пустых строк.\n",
        "        save_to_file(text_out, 'fixed_markdown_6.md')\n",
        "    if is_convert_to_ordered_list:\n",
        "        text_out = convert_lists_to_ordered(text_out)\n",
        "        save_to_file(text_out, 'fixed_markdown_7.md')\n",
        "    if is_renumber_headings:\n",
        "        text_out = renumber_headings(text_out)\n",
        "        save_to_file(text_out, 'fixed_markdown_8.md')\n",
        "    if is_copy_headings_into_content:\n",
        "        text_out = copy_headings_into_content(text_out)\n",
        "        save_to_file(text_out, 'fixed_markdown_9.md')\n",
        "    if is_add_token_counts:\n",
        "        text_out = add_token_counts_to_headings(text_out)\n",
        "        save_to_file(text_out, 'fixed_markdown_10.md')\n",
        "\n",
        "    save_to_file(text_out, 'fixed_markdown_11.md')\n",
        "\n",
        "\n",
        "\n",
        "    return text_out\n",
        "\n",
        "text = load_file_of_url(link_bd_text)\n",
        "text_preproces = preprocessing(text, token_min, token_max )\n"
      ],
      "metadata": {
        "id": "9sR-pOIWkA1K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "cellView": "form",
        "outputId": "d2732a50-1e4d-45cf-a9c4-113745e7e8f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Исправленный текст сохранен в файл fixed_markdown_1.md\n",
            "\n",
            "\n",
            "***** Удаление пустых строк *****\n",
            "\n",
            "***** Обработчик правильности уровней вложенности *****\n",
            "\n",
            "Исправление: Заголовок Что такое chatGPT\r\n",
            "                                       с уровня 4 заменен на уровень 2.\n",
            "Исправление: Заголовок Другие возможности ChatGPT:\r\n",
            "                                       с уровня 4 заменен на уровень 2.\n",
            "Исправление: Заголовок В связи с появлением ChatGPT появятся несколько новых профессий:\r\n",
            "                                       с уровня 4 заменен на уровень 3.\n",
            "Исправление: Заголовок Где можно применять дообучение ChatGPT:\r\n",
            "                                       с уровня 4 заменен на уровень 3.\n",
            "Исправление: Заголовок Примеры возможностей ChatGPT:\r\n",
            "                                       с уровня 4 заменен на уровень 3.\n",
            "Исправление: Заголовок ChatGPT может выполнять такие функции, как:\r\n",
            "                                       с уровня 4 заменен на уровень 3.\n",
            "Исправление: Заголовок Преимущества использования ChatGPT:\r\n",
            "                                       с уровня 4 заменен на уровень 3.\n",
            "Исправление: Заголовок ChatGPT трансформирует рынок труда\r\n",
            "                                       с уровня 4 заменен на уровень 3.\n",
            "Исправление: Заголовок Факты о ChatGPT:\r\n",
            "                                       с уровня 4 заменен на уровень 3.\n",
            "Исправление: Заголовок ChatGPT - самая обсуждаемая тема в медиа 2023 года:\r\n",
            "                                       с уровня 4 заменен на уровень 3.\n",
            "Исправление: Заголовок Крупные компании, которые внедряют ChatGPT:\r\n",
            "                                       с уровня 4 заменен на уровень 3.\n",
            "Исправление: Заголовок Результаты опроса россиян на предмет осведомленности о возможностях ChatGPT:\r\n",
            "                                       с уровня 4 заменен на уровень 3.\n",
            "Исправление: Заголовок Как реализуется дообучение ChatGPT:\r\n",
            "                                       с уровня 4 заменен на уровень 3.\n",
            "Исправление: Заголовок Стоимость использование chatGPT\r\n",
            "                                       с уровня 4 заменен на уровень 3.\n",
            "Исправление: Заголовок Как тестируется СhatGPT:\r\n",
            "                                       с уровня 4 заменен на уровень 3.\n",
            "Исправление: Заголовок Как происходит тестирование и исправление вопросов до запуска бота (тестовый период):\r\n",
            "                                       с уровня 5 заменен на уровень 3.\n",
            "Исправление: Заголовок Как происходит тестирование вопросов после запуска бота: \r\n",
            "                                       с уровня 5 заменен на уровень 3.\n",
            "Исправление: Заголовок Как улучшить ответы ChatGPT:\r\n",
            "                                       с уровня 5 заменен на уровень 3.\n",
            "Исправление: Заголовок Какие есть роли в работе с ChatGPT:\r\n",
            "                                       с уровня 4 заменен на уровень 3.\n",
            "Исправление: Заголовок Модель математического нейрона Маккаллока — Питтса\r\n",
            "                                       с уровня 4 заменен на уровень 3.\n",
            "Исправление: Заголовок Функции активации\r\n",
            "                                       с уровня 4 заменен на уровень 3.\n",
            "Исправленный текст сохранен в файл fixed_markdown_2.md\n",
            "\n",
            "\n",
            "***** Создание оглавления. *****\n",
            "\n",
            "Исправленный текст сохранен в файл toc.md\n",
            "\n",
            "***** Расширение заголовков *****\n",
            "\n",
            "Исправленный текст сохранен в файл fixed_markdown_3.md\n",
            "\n",
            "***** Разделение разделов на части *****\n",
            "\n",
            "## Описание УИИ > История УИИ - 3575 токенов\n",
            "     ## Описание УИИ > История УИИ - part 1 - 2118 токенов\n",
            "     ## Описание УИИ > История УИИ - part 2 - 1458 токенов\n",
            "# Чем УИИ лучше конкурентов: - 4331 токенов\n",
            "     # Чем УИИ лучше конкурентов: - part 1 - 2544 токенов\n",
            "     # Чем УИИ лучше конкурентов: - part 2 - 1788 токенов\n",
            "## Трудоустройство > Примеры трудоустройства выпускников УИИ: - 8404 токенов\n",
            "     ## Трудоустройство > Примеры трудоустройства выпускников УИИ: - part 1 - 2348 токенов\n",
            "     ## Трудоустройство > Примеры трудоустройства выпускников УИИ: - part 2 - 2279 токенов\n",
            "     ## Трудоустройство > Примеры трудоустройства выпускников УИИ: - part 3 - 2550 токенов\n",
            "     ## Трудоустройство > Примеры трудоустройства выпускников УИИ: - part 4 - 1228 токенов\n",
            "## Стажировки в УИИ > Список стажировок для новых студентов УИИ - 7324 токенов\n",
            "     ## Стажировки в УИИ > Список стажировок для новых студентов УИИ - part 1 - 2469 токенов\n",
            "     ## Стажировки в УИИ > Список стажировок для новых студентов УИИ - part 2 - 2398 токенов\n",
            "     ## Стажировки в УИИ > Список стажировок для новых студентов УИИ - part 3 - 2458 токенов\n",
            "## Стажировки в УИИ > Список стажировок для действующих студентов УИИ - 7841 токенов\n",
            "     ## Стажировки в УИИ > Список стажировок для действующих студентов УИИ - part 1 - 2287 токенов\n",
            "     ## Стажировки в УИИ > Список стажировок для действующих студентов УИИ - part 2 - 2414 токенов\n",
            "     ## Стажировки в УИИ > Список стажировок для действующих студентов УИИ - part 3 - 2309 токенов\n",
            "     ## Стажировки в УИИ > Список стажировок для действующих студентов УИИ - part 4 - 832 токенов\n",
            "## Хакатоны > Студенты УИИ регулярно принимают участие в хакатонах и занимают призовые места - 5450 токенов\n",
            "     ## Хакатоны > Студенты УИИ регулярно принимают участие в хакатонах и занимают призовые места - part 1 - 2539 токенов\n",
            "     ## Хакатоны > Студенты УИИ регулярно принимают участие в хакатонах и занимают призовые места - part 2 - 2519 токенов\n",
            "     ## Хакатоны > Студенты УИИ регулярно принимают участие в хакатонах и занимают призовые места - part 3 - 393 токенов\n",
            "## Вебинары УИИ > Вебинар “Вселенная AI 2023. День 1” - 2643 токенов\n",
            "     ## Вебинары УИИ > Вебинар “Вселенная AI 2023. День 1” - part 1 - 2311 токенов\n",
            "     ## Вебинары УИИ > Вебинар “Вселенная AI 2023. День 1” - part 2 - 333 токенов\n",
            "## Описание AI > Мировые новости говорят об Искусственном Интеллекте и его возможностях: - 42448 токенов\n",
            "     ## Описание AI > Мировые новости говорят об Искусственном Интеллекте и его возможностях: - part 1 - 2345 токенов\n",
            "     ## Описание AI > Мировые новости говорят об Искусственном Интеллекте и его возможностях: - part 2 - 2484 токенов\n",
            "     ## Описание AI > Мировые новости говорят об Искусственном Интеллекте и его возможностях: - part 3 - 2512 токенов\n",
            "     ## Описание AI > Мировые новости говорят об Искусственном Интеллекте и его возможностях: - part 4 - 2532 токенов\n",
            "     ## Описание AI > Мировые новости говорят об Искусственном Интеллекте и его возможностях: - part 5 - 2269 токенов\n",
            "     ## Описание AI > Мировые новости говорят об Искусственном Интеллекте и его возможностях: - part 6 - 2317 токенов\n",
            "     ## Описание AI > Мировые новости говорят об Искусственном Интеллекте и его возможностях: - part 7 - 2061 токенов\n",
            "     ## Описание AI > Мировые новости говорят об Искусственном Интеллекте и его возможностях: - part 8 - 2459 токенов\n",
            "     ## Описание AI > Мировые новости говорят об Искусственном Интеллекте и его возможностях: - part 9 - 2541 токенов\n",
            "     ## Описание AI > Мировые новости говорят об Искусственном Интеллекте и его возможностях: - part 10 - 2431 токенов\n",
            "     ## Описание AI > Мировые новости говорят об Искусственном Интеллекте и его возможностях: - part 11 - 2395 токенов\n",
            "     ## Описание AI > Мировые новости говорят об Искусственном Интеллекте и его возможностях: - part 12 - 2269 токенов\n",
            "     ## Описание AI > Мировые новости говорят об Искусственном Интеллекте и его возможностях: - part 13 - 2474 токенов\n",
            "     ## Описание AI > Мировые новости говорят об Искусственном Интеллекте и его возможностях: - part 14 - 2347 токенов\n",
            "     ## Описание AI > Мировые новости говорят об Искусственном Интеллекте и его возможностях: - part 15 - 2512 токенов\n",
            "     ## Описание AI > Мировые новости говорят об Искусственном Интеллекте и его возможностях: - part 16 - 2157 токенов\n",
            "     ## Описание AI > Мировые новости говорят об Искусственном Интеллекте и его возможностях: - part 17 - 2465 токенов\n",
            "     ## Описание AI > Мировые новости говорят об Искусственном Интеллекте и его возможностях: - part 18 - 1879 токенов\n",
            "# Библиотеки по нейронным сетям, с которыми работает УИИ: - 6017 токенов\n",
            "     # Библиотеки по нейронным сетям, с которыми работает УИИ: - part 1 - 1756 токенов\n",
            "     # Библиотеки по нейронным сетям, с которыми работает УИИ: - part 2 - 2512 токенов\n",
            "     # Библиотеки по нейронным сетям, с которыми работает УИИ: - part 3 - 1749 токенов\n",
            "## Общая информация о chatGPT > 20 примеров нейро-сотрудников - 2858 токенов\n",
            "     ## Общая информация о chatGPT > 20 примеров нейро-сотрудников - part 1 - 2526 токенов\n",
            "     ## Общая информация о chatGPT > 20 примеров нейро-сотрудников - part 2 - 332 токенов\n",
            "\n",
            "***** Удаление пустых строк *****\n",
            "\n",
            "Исправленный текст сохранен в файл fixed_markdown_4.md\n",
            "\n",
            "***** Объединяет разделы у которых есть единственный дочерний элемент *****\n",
            "\n",
            "Обработан раздел - ## Прохождение курсов > Как найти время на прохождение курса: > Варианты удобного графика прохождения курса для студентов УИИ:. (819 + 770) 1647 токенов\n",
            "Обработан раздел - ## Актуальные Курсы УИИ > Описание основных курсов УИИ > Список основных курсов УИИ:. (199 + 27) 264 токенов\n",
            "Обработан раздел - ## Описание дополнительных курсов УИИ > Углубленный курс по текстам (NLP) > Темы курса:. (39 + 89) 166 токенов\n",
            "Обработан раздел - ### Описание дополнительных курсов УИИ > Terra AI Laboratory > Разбор аутсорс проекта \"Подсчет входящих/выходящих людей из автобуса\". Проект \"Подсчет входящих/выходящих людей из автобуса\" – самый крупный AI проект УИИ как аутсорсинговой компании. > Этапы проекта:. (176 + 987) 1287 токенов\n",
            "Обработан раздел - ## Описание дополнительных курсов УИИ > Общая информация о курсах по ChatGPT УИИ: > Описание курсов по ChatGPT. (0 + 1132) 1176 токенов\n",
            "Обработан раздел - ## Описание дополнительных курсов УИИ > Описание курса по продаже аутсорса > Состав курса:. (49 + 421) 508 токенов\n",
            "Обработан раздел - ## Описание дополнительных курсов УИИ > Описание курса по SQL > Программа курса:. (0 + 397) 427 токенов\n",
            "Обработан раздел - ### Тарифы > Список тарифов курса Data Science и нейтронные сети с ценами и программой курса: > Тариф “Базовый” > Тариф \"Базовый\" включает:. (180 + 1523) 1766 токенов\n",
            "Обработан раздел - ### Тарифы > Список тарифов курса Data Science и нейтронные сети с ценами и программой курса: > Тариф “Продвинутый” > Состав тарифа \"Продвинутый\":. (271 + 1003) 1342 токенов\n",
            "Обработан раздел - ### Тарифы > Список тарифов курса Data Science и нейтронные сети с ценами и программой курса: > Тариф \"AI под ключ\" > Тариф “AI под ключ”:. (154 + 985) 1197 токенов\n",
            "Обработан раздел - ### Тарифы > Тарифы курса ChatGPT Professional: > Тариф “Professional light” > Занятия тарифа:. (46 + 902) 986 токенов\n",
            "Обработан раздел - ### Тарифы > Тарифы курса ChatGPT Professional: > Тариф “Professional Pro” > Занятия тарифа:. (46 + 938) 1022 токенов\n",
            "Обработан раздел - ## Аутсорс проекты > Как выбрать тему AI проекта > Два ключевых вопроса, которые позволяют выбрать тему проекта:. (434 + 66) 549 токенов\n",
            "Обработан раздел - ## Разработка AI-проекта для компаний о УИИ > Внедрение нейро-сотрудников а бизнес, создание AI-проекта для бизнеса от УИИ бесплатный вариант > Как выбрать нейро-сотрудника для внедрения в бизнес. (843 + 742) 1685 токенов\n",
            "Обработан раздел - ## Стажировки в УИИ > Как приобрести стажировки > Стоимость участия в стажировке. (69 + 265) 370 токенов\n",
            "Обработан раздел - ## Стажировки в УИИ > Стажировка по проекту: “Определение класса шпона” от ООО “Мануфактура” > Команда участников:. (431 + 758) 1245 токенов\n",
            "Обработан раздел - ## Стажировки в УИИ > Стажировка по проекту: “Определение контрагента по записи в 1С” от ООО “1С-РАРУС” > Команда участников:. (362 + 1425) 1850 токенов\n",
            "Обработан раздел - ## Стажировки в УИИ > Стажировка с object detection по проекту: “Обнаружение людей, акул и нефтяных пятен с квадрокоптера” от ООО “Дрон Солюшнс” > Участники проекта:. (783 + 926) 1790 токенов\n",
            "Обработан раздел - ## Стажировки в УИИ > Стажировка по проекту “Система сегментации грудного объема и плеврального выпота в пораженных легких” от ООО “РАДЛОДЖИКС РУС” > Участники проекта:. (381 + 669) 1135 токенов\n",
            "Обработан раздел - ## Команда УИИ > Структура отделов УИИ > Кураторы курса «Data science и нейронные сети»:. (992 + 1052) 2086 токенов\n",
            "Обработан раздел - # Оплата курса картами иностранных банков > Из каких стран принимается оплата. (104 + 408) 545 токенов\n",
            "Обработан раздел - ## Вебинары УИИ > Вебинары по данным > Темы по подготовке датасетов:. (208 + 497) 738 токенов\n",
            "Обработан раздел - ### Вебинары УИИ > Вебинар “Вселенная ChatGPT. День 1” > Нейро-консультант на основе ChatGPT - стажировка для KIA > Что говорят студенты о стажировках ChatGPT:. (265 + 472) 815 токенов\n",
            "Обработан раздел - ### Вебинары УИИ > Вебинар “Вселенная ChatGPT. День 1” > Проект №2. Нейро-экзаменатор по учебным материалам - стажировка для Platrum. > Что говорят студенты о стажировках ChatGPT:. (131 + 650) 869 токенов\n",
            "Обработан раздел - ## Вебинары УИИ > Вебинар по ChatGPT > Темы вебинара:. (297 + 192) 515 токенов\n",
            "Обработан раздел - ## Вебинары УИИ > Вебинар по AutoML > Программа вебинара:. (189 + 62) 276 токенов\n",
            "Обработан раздел - ## Вебинары УИИ > Вебинар по датасетам > Программа вебинара:. (969 + 76) 1073 токенов\n",
            "Обработан раздел - ## Вебинары УИИ > Вебинар об истории УИИ > На вебинаре студенты УИИ получат информацию по следующим темам:. (0 + 191) 240 токенов\n",
            "Обработан раздел - ## Вебинары УИИ > 4 вебинара по созданию нейронных сетей на Keras с разбором реального AI проекта > Темы вебинаров:. (79 + 114) 244 токенов\n",
            "Обработан раздел - ## Вебинары УИИ > Вебинар по трудоустройству > Студенты УИИ получат информацию по следующим темам:. (0 + 86) 130 токенов\n",
            "Обработан раздел - ## Вебинары УИИ > Марафон вебинаров по теме “Профессия AI-разработчик\" > Студенты УИИ получат информацию по следующим темам:. (0 + 134) 191 токенов\n",
            "Обработан раздел - ## Вебинары УИИ > Вебинар по разбору 10 новых нейронных сетей из демо-панели > Области применения обучения с подкреплением:. (367 + 62) 487 токенов\n",
            "Обработан раздел - ## Вебинары УИИ > Вебинар про два ключевых направления в современном AI > Студенты УИИ получат информацию по следующим темам:. (30 + 109) 190 токенов\n",
            "Обработан раздел - ## Вебинары УИИ > Вебинар “Ключевые задачи в AI” > Студенты УИИ получат информацию по следующим темам. (0 + 86) 132 токенов\n",
            "Обработан раздел - ## Вебинары УИИ > Вебинар” Обзор 5 AutoML фреймворков в прямом эфире” > Вебинар состоит из нескольких частей:. (62 + 155) 274 токенов\n",
            "Обработан раздел - ## Вебинары УИИ > Вебинар AI реалити > Студенты УИИ получат информацию по следующим темам:. (0 + 100) 141 токенов\n",
            "Обработан раздел - ## Вебинары УИИ > Вебинар “Создание нейро-копирайтера” > Создание нейро-копирайтера:. (50 + 229) 325 токенов\n",
            "Обработан раздел - ## Вебинары УИИ > 2 вебинара по дообучению ChatGPT > Что может ChatGPT:. (39 + 595) 667 токенов\n",
            "Обработан раздел - ## Вебинары УИИ > Вебинар “AI на фреймворках” > Темы вебинара:. (73 + 264) 371 токенов\n",
            "Обработан раздел - ## Вебинары УИИ > Вебинар по продаже проектов по ChatGPT на заказ > Темы вебинара:. (811 + 96) 942 токенов\n",
            "Обработан раздел - ## Вебинары УИИ > Вебинар разбор AutoML фреймворков: Fedot и AutoTS для временных рядов. > 5 основных типовых блока под временные ряды:. (198 + 1017) 1273 токенов\n",
            "Обработан раздел - ## Вебинары УИИ > Вебинар по дообучению  chatGPT > Темы вебинара:. (0 + 322) 354 токенов\n",
            "Обработан раздел - ## Вебинары УИИ > Вебинар: создание AI на фреймворках > Темы вебинара:. (0 + 244) 279 токенов\n",
            "Обработан раздел - ## Вебинары УИИ > Вебинар по AutoML > На вебинаре будут разбираться 3 библиотеки:. (0 + 131) 171 токенов\n",
            "Обработан раздел - ## Вебинары УИИ > Вебинар про создание tg-ботов и android-приложения для нейронок. > Темы вебинара:. (96 + 115) 255 токенов\n",
            "Обработан раздел - ## Вебинары УИИ > Вебинар: создание нейронных сетей на фреймворках. > Темы вебинара:. (17 + 533) 594 токенов\n",
            "Обработан раздел - ## Вебинары УИИ > Вебинар по датасетам > Темы вебинара:. (278 + 134) 440 токенов\n",
            "Обработан раздел - ## Вебинары УИИ > Вебинар: Дообучение chatGPT: создание нейро-куратора > Темы вебинара:. (0 + 169) 212 токенов\n",
            "Обработан раздел - ## Вебинары УИИ > Вебинар по продаже Al проектов на заказ > Темы вебинара:. (0 + 262) 294 токенов\n",
            "Обработан раздел - ## Вебинары УИИ > Вебинар с разбором нейро-сотрудников > Нейро-сотрудники, которые будут разбираться на вебинаре:. (12 + 220) 290 токенов\n",
            "Обработан раздел - # Terra AI Universe > Программа Terra AI Universe. (31 + 0) 43 токенов\n",
            "Обработан раздел - ## Реалити > Реалити: Создание датасета по определению заболеваний сердца > Команда разработчиков:. (115 + 84) 245 токенов\n",
            "Обработан раздел - # Интенсивы > Интенсив по AI. (123 + 0) 141 токенов\n",
            "Обработан раздел - ### Конкурс идей AI проектов > Двухдневный Интенсив по AI > Расписание интенсива: > День 1:. (0 + 90) 137 токенов\n",
            "Обработан раздел - # Стратсессия > Интерактивная методичка, разработанная УИИ. (0 + 0) 25 токенов\n",
            "Обработан раздел - ## Помощь экспертов УИИ в создании AI-проекта студента: > AI проекты, созданные студентами УИИ > Прочие темы проектов, созданные студентами УИИ:. (1680 + 682) 2428 токенов\n",
            "Обработан раздел - ## Помощь экспертов УИИ в создании AI-проекта студента: > Как устроены нейронные сети: > Слои нейронной сети:. (821 + 520) 1398 токенов\n",
            "Обработан раздел - # Детальный разбор AI-проектов > Проекты:. (336 + 2430) 2788 токенов\n",
            "Обработан раздел - # Сегментация изображений с помощью предобучений модели MASK RCNN > Задача обнаружения объектов может быть выполнена с помощью двух основных подходов:. (134 + 814) 999 токенов\n",
            "Обработан раздел - # Keras Tuner > Подходы работы Keras Tuner:. (220 + 314) 550 токенов\n",
            "Обработан раздел - # Что умеет чат-бот УИИ > Чат-бот умеет:. (108 + 566) 702 токенов\n",
            "Обработан раздел - ### УИИ специализируется на внедрении нейро-сотрудников > 7 проектов нейро-сотрудников на chatGPT > Нейро-трафиколог > Замена chatGPTна контурные модели. (701 + 84) 861 токенов\n",
            "Обработан раздел - # Пакет GPT Practic > Условия приобретения пакета GPT Practic:. (200 + 46) 274 токенов\n",
            "Обработан раздел - # Прохождение программы на основных курсах УИИ по депозиту > Пакет депозита и цены:. (285 + 263) 588 токенов\n",
            "Исправленный текст сохранен в файл fixed_markdown_5.md\n",
            "\n",
            "\n",
            "***** Удаление пустых строк *****\n",
            "\n",
            "Исправленный текст сохранен в файл fixed_markdown_6.md\n",
            "\n",
            "\n",
            "***** Приведение всех списков к единому виду нумерованных списков. *****\n",
            "\n",
            "Исправленный текст сохранен в файл fixed_markdown_7.md\n",
            "\n",
            "\n",
            "***** Нумерации заголовков. *****\n",
            "\n",
            "Исправленный текст сохранен в файл fixed_markdown_8.md\n",
            "\n",
            "\n",
            "***** Копирование заголовков в текст соответствующих глав *****\n",
            "\n",
            "Исправленный текст сохранен в файл fixed_markdown_9.md\n",
            "\n",
            "***** Добавление количества токенов к заголовку *****\n",
            "\n",
            "Исправленный текст сохранен в файл fixed_markdown_10.md\n",
            "\n",
            "Исправленный текст сохранен в файл fixed_markdown_11.md\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25x9IUpWwIbm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "5c868d4a-8bed-4965-fc25-e0fad601fb26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Первая стадия разбивки: MarkdownHeaderTextSplitter. Число фрагментов: 916 шт.\n",
            "Вторая стадия разбивки: Semantic-Text-Splitter(HuggingFaceTextSplitter). Число фрагментов: 2169 шт.\n"
          ]
        }
      ],
      "source": [
        "# @title  Выбор сплитеров\n",
        "\n",
        "first_stage_split = \"MarkdownHeaderTextSplitter\" # @param [\"MarkdownHeaderTextSplitter\", \"RecursiveCharacterTextSplitter\", \"Semantic-Text-Splitter(HuggingFaceTextSplitter)\", \"Semantic Chunking (Greg Kamradt)\"]\n",
        "second_stage_split = \"Semantic-Text-Splitter(HuggingFaceTextSplitter)\" # @param [\"None\", \"RecursiveCharacterTextSplitter\", \"Semantic-Text-Splitter(HuggingFaceTextSplitter)\", \"Semantic Chunking (Greg Kamradt)\"]\n",
        "recursive_chunk_size = 1060 # @param {type:\"number\"}\n",
        "recursive_chunk_overlap = 0 # @param {type:\"number\"}\n",
        "hf_semantic_chunk_size_min = 350 # @param {type:\"number\"}\n",
        "hf_semantic_chunk_size_max = 1060 # @param {type:\"number\"}\n",
        "gk_semantic_breakpoint = \"percentile\" # @param [\"percentile\", \"standard_deviation\", \"interquartile\"]\n",
        "\n",
        "# percentile - The default way to split is based on percentile. In this method,\n",
        "# all differences between sentences are calculated, and then any difference greater than the X percentile is split.\n",
        "# standard_deviation - In this method, any difference greater than X standard deviations is split.\n",
        "# interquartile - in this method, the interquartile distance is used to split chunks\n",
        "\n",
        "# Разбивка на чанки.\n",
        "docs =  split_text( text=text_preproces,\n",
        "                    first_stage_split=first_stage_split,\n",
        "                    second_stage_split=second_stage_split,\n",
        "                    chunk_size=int(recursive_chunk_size),\n",
        "                    chunk_overlap=recursive_chunk_overlap,\n",
        "                    length_function='token',\n",
        "                    semantic_chunk_size_min=hf_semantic_chunk_size_min,\n",
        "                    semantic_chunk_size_max=hf_semantic_chunk_size_max,\n",
        "                    semantic_breakpoint=gk_semantic_breakpoint)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "id": "jnDu1p4LyFEI",
        "outputId": "6cb3bd78-c333-482f-aec1-2cf509f81f79",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Число чанков: 2169\n",
            "{'len': 584, 'page_content': '**49.13 Описание AI > Мировые новости говорят об Искусственном Интеллекте и его возможностях: - part 8**\\n1. Астрахань 24: “В ближайшие 5 лет самыми востребованными на рынке станут эксперты по ИИ”. ИИ будет определять наше будущее, а самыми востребованными специалистами в ближайшие пять лет станут эксперты по ИИ и машинному обучению, бизнес-аналитики и инженеры по робототехнике. Об этом в своей лекции рассказал первый зампред Правления Сбербанка Александр Ведяхин. Спикер отметил, что роль ИИ сегодня сложно переоценить. ИИ меняет будущее, применяется во всех сферах экономики и жизни и активно развивается. Топовые мировые конференции и научные исследования всё чаще посвящаются теме искусственного интеллекта. Если в 2010 году вышло 200 тыс. публикаций на эту тему, то в 2021-м уже почти 500 тыс. - рост в 2,5 раза за 10 лет. Как показывают опросы, 94 % компаний уверены, что ИИ способствует успеху организации. Он применим абсолютно во всех сферах, внедряется в сервисы и создаёт новые миры. Например, генерирует дизайн (двигателей, зданий и др.), синтезирует новые материалы, создаёт 3D-карты городов, презентации и лекарства, занимается расшифровкой генома и мониторингом урожая.\\n'}\n",
            "[584, 582, 573, 572, 567, 559, 549, 545, 525, 519, 513, 510, 510, 505, 497, 492, 492, 492, 491, 490, 485, 476, 475, 475, 472, 469, 467, 462, 460, 458, 455, 452, 451, 449, 449, 446, 446, 445, 444, 444, 440, 436, 435, 434, 434, 434, 430, 430, 429, 428]\n",
            "[7, 8, 8, 8, 9, 9, 9, 10, 10, 10, 10, 11, 11, 12, 12, 12, 12, 13, 13, 14, 14, 14, 14, 15, 15, 16, 16, 16, 16, 16, 16, 17, 17, 17, 18, 18, 18, 19, 19, 19, 19, 20, 20, 20, 21, 21, 21, 21, 22, 23]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='**1 Описание УИИ**\\nУниверситет Искусственного Интеллекта - ведущее образовательное учреждение в России, специализирующееся исключительно на области искусственного интеллекта. Он\n",
            "является самым крупным университетом по ИИ в России и странах СНГ, признанным лидером в образовании в сфере ИИ.\\nПараллельно с созданием курсов на протяжении пяти лет УИИ активно занимается\n",
            "разработкой в AI-сфере: создает AI-проекты для компаний на заказ.\\n' metadata={'H1': '1 Описание УИИ (452 токенов)'}\n",
            "\n",
            "page_content='УИИ не имеет филиалов за границей.\\nКурсы в УИИ проходят студенты по всему миру.\\nБолее 4700 студентов со всего мира выбрали УИИ для прохождения курса по AI. Они получают не только\n",
            "теоретические знания, но и активно применяют их в создании проектов по искусственному интеллекту. Благодаря этому, студентами УИИ было создано уже 372 проекта в области ИИ.\\nУниверситет также\n",
            "предоставляет консультации более чем 100 крупным и малым компаниям, помогая им реализовывать и внедрять искусственный интеллект в своей деятельности.' metadata={'H1': '1 Описание УИИ (452 токенов)'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title Диаграмма\n",
        "\n",
        "# Посмотреть число чанков и их размер.\n",
        "if True :\n",
        "    # # Получение списка количества токенов в чанках.\n",
        "    list_count = []\n",
        "    list_count_plot = []\n",
        "    for i, doc in enumerate(docs):\n",
        "        list_count.append({\"len\":num_tokens_from_string(doc.page_content), \"page_content\": doc.page_content})\n",
        "        list_count_plot.append(num_tokens_from_string(doc.page_content))\n",
        "    # Изменить направление сортировки - от большего к меньшему.\n",
        "    list_count_sort = sorted(list_count, key=lambda x: x[\"len\"], reverse=True)[:10]\n",
        "    print(f\"Число чанков: {len(list_count)}\")\n",
        "    print(list_count_sort[0])\n",
        "    a = sorted(list_count_plot, reverse=True)[:50]\n",
        "    print(a)\n",
        "    b = sorted(list_count_plot, reverse=False)[:50]\n",
        "    print(b)\n",
        "\n",
        "\n",
        "    # Подсчет токенов для каждого source_chunk и построение графика\n",
        "    source_chunk_token_counts = list_count_plot\n",
        "    plt.hist(source_chunk_token_counts, bins=200, alpha=0.5, label='Source Chunks')\n",
        "    plt.title('Distribution of Source Chunk Token Counts')\n",
        "    plt.xlabel('Token Count')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()\n",
        "\n",
        "    for doc in docs[:2]:\n",
        "        print(fill(str(doc), width=200))\n",
        "        print()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOQKWK5QL3BqHD1AWIK+Iek",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}